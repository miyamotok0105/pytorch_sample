{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"AllenNLPで簡単にテキストを学習する.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":["08BafNHdl5xg"]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"5azIyHMyk0G_","colab_type":"text"},"cell_type":"markdown","source":["https://github.com/allenai/allennlp/blob/v0.5.0/README.md\n","\n","\n","https://github.com/allenai/allennlp/blob/v0.5.0/tutorials/getting_started/training_and_evaluating.md\n","\n","先にこれをやった。\n","\n","https://medium.com/swlh/deep-learning-for-text-made-easy-with-allennlp-62bc79d41f31"]},{"metadata":{"id":"YbZsfpN9mE8p","colab_type":"text"},"cell_type":"markdown","source":["# Install allennlp and PyTorch"]},{"metadata":{"id":"MDF9E93hk510","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":289},"outputId":"b0c0ec41-8ccb-45a8-bba0-f3924ee04352","executionInfo":{"status":"ok","timestamp":1533282261245,"user_tz":-540,"elapsed":9072,"user":{"displayName":"宮本圭一郎","photoUrl":"//lh5.googleusercontent.com/-5BLtx8oPSy8/AAAAAAAAAAI/AAAAAAAALtI/-tIwIsmAvCs/s50-c-k-no/photo.jpg","userId":"100227668169464343249"}}},"cell_type":"code","source":["!pip install torch==0.4.0\n","!pip install torchvision==0.2.1"],"execution_count":5,"outputs":[{"output_type":"stream","text":["Requirement already satisfied: torch==0.4.0 in /usr/local/lib/python3.6/dist-packages (0.4.0)\n","Collecting torchvision==0.2.1\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/0d/f00b2885711e08bd71242ebe7b96561e6f6d01fdb4b9dcf4d37e2e13c5e1/torchvision-0.2.1-py2.py3-none-any.whl (54kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 2.5MB/s \n","\u001b[?25hRequirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (1.14.5)\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (1.11.0)\n","Collecting pillow>=4.1.1 (from torchvision==0.2.1)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d1/24/f53ff6b61b3d728b90934bddb4f03f8ab584a7f49299bf3bde56e2952612/Pillow-5.2.0-cp36-cp36m-manylinux1_x86_64.whl (2.0MB)\n","\u001b[K    100% |████████████████████████████████| 2.0MB 7.4MB/s \n","\u001b[?25hRequirement already satisfied: torch in /usr/local/lib/python3.6/dist-packages (from torchvision==0.2.1) (0.4.0)\n","Installing collected packages: pillow, torchvision\n","  Found existing installation: Pillow 4.0.0\n","    Uninstalling Pillow-4.0.0:\n","      Successfully uninstalled Pillow-4.0.0\n","Successfully installed pillow-5.2.0 torchvision-0.2.1\n"],"name":"stdout"}]},{"metadata":{"id":"sZVuTK31kxVg","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":2674},"outputId":"31bb9e6b-cd54-4921-82b8-7af4dc671b75","executionInfo":{"status":"ok","timestamp":1533282252035,"user_tz":-540,"elapsed":84420,"user":{"displayName":"宮本圭一郎","photoUrl":"//lh5.googleusercontent.com/-5BLtx8oPSy8/AAAAAAAAAAI/AAAAAAAALtI/-tIwIsmAvCs/s50-c-k-no/photo.jpg","userId":"100227668169464343249"}}},"cell_type":"code","source":["!pip install allennlp"],"execution_count":4,"outputs":[{"output_type":"stream","text":["Collecting allennlp\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8d/34/1439e851e8ebd913f958b40c48bc75451d37f337105703fa04dd855ae3f5/allennlp-0.5.1-py3-none-any.whl (3.9MB)\n","\u001b[K    100% |████████████████████████████████| 3.9MB 7.9MB/s \n","\u001b[?25hRequirement already satisfied: scikit-learn in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.19.2)\n","Requirement already satisfied: h5py in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.8.0)\n","Collecting tensorboardX==1.2 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/c5/22/43f4f0318f7c68a1000dbb700a353b745584bc2397437832d15ba69ea5f1/tensorboardX-1.2-py2.py3-none-any.whl (44kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 21.0MB/s \n","\u001b[?25hCollecting pyhocon==0.3.35 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/95/b9/72883593ce531e95ac8190e251ae6f5377eada69504248bf1aebfce4c5b4/pyhocon-0.3.35.tar.gz (94kB)\n","\u001b[K    100% |████████████████████████████████| 102kB 27.4MB/s \n","\u001b[?25hCollecting gevent==1.2.2 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/af/438c505b62f630c203a6a6a5a3b6fec49524c0cda7ab6392be03999537ad/gevent-1.2.2-cp36-cp36m-manylinux1_x86_64.whl (1.7MB)\n","\u001b[K    100% |████████████████████████████████| 1.7MB 15.1MB/s \n","\u001b[?25hCollecting flaky (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/f8/92/4f16fc920fb3b58233a2b791edee8934e8644907bc9966c099570a7e46b1/flaky-3.4.0-py2.py3-none-any.whl\n","Requirement already satisfied: requests>=2.18 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.18.4)\n","Collecting pytz==2017.3 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/a3/7f/e7d1acbd433b929168a4fb4182a2ff3c33653717195a26c1de099ad1ef29/pytz-2017.3-py2.py3-none-any.whl (511kB)\n","\u001b[K    100% |████████████████████████████████| 512kB 19.6MB/s \n","\u001b[?25hRequirement already satisfied: nltk in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.2.5)\n","Requirement already satisfied: typing in /usr/local/lib/python3.6/dist-packages (from allennlp) (3.6.4)\n","Collecting numpydoc==0.8.0 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/95/a8/b4706a6270f0475541c5c1ee3373c7a3b793936ec1f517f1a1dab4f896c0/numpydoc-0.8.0.tar.gz\n","Collecting torch==0.4.0 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/69/43/380514bd9663f1bf708abeb359b8b48d3fabb1c8e95bb3427a980a064c57/torch-0.4.0-cp36-cp36m-manylinux1_x86_64.whl (484.0MB)\n","\u001b[K    10% |███▎                            | 49.7MB 32.8MB/s eta 0:00:14"],"name":"stdout"},{"output_type":"stream","text":["\u001b[K    100% |████████████████████████████████| 484.0MB 25kB/s \n","tcmalloc: large alloc 1073750016 bytes == 0x5bb3c000 @  0x7f7679b8d1c4 0x46d6a4 0x5fcbcc 0x4c494d 0x54f3c4 0x553aaf 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54efc1 0x54f24d 0x551ee0 0x54efc1 0x54f24d 0x551ee0 0x54e4c8 0x54f4f6 0x553aaf 0x54e4c8\n","\u001b[?25hCollecting conllu (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/e2/61/508e88e2ee979ce6bdf096f0e0ef3e2a6c9c0fe8b37b06bebe7c4095e536/conllu-0.10.6-py2.py3-none-any.whl\n","Collecting editdistance (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/14/8f/fb22110ea5143632e7ba4346106277e7a7a5dc6f5d37d8feae7fabd755ae/editdistance-0.4-cp36-cp36m-manylinux1_x86_64.whl (174kB)\n","\u001b[K    100% |████████████████████████████████| 184kB 19.1MB/s \n","\u001b[?25hRequirement already satisfied: tqdm>=4.19 in /usr/local/lib/python3.6/dist-packages (from allennlp) (4.24.0)\n","Collecting flask-cors==3.0.3 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/83/a7/c7243ffd096a491013956c9ee71e2ed0b7d14979fafe89986ca2d30fc6f7/Flask_Cors-3.0.3-py2.py3-none-any.whl\n","Collecting awscli>=1.11.91 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/99/a1a1ea5a91161d5be5f434550ac1de800d79da7d4f68a5b5c8f265fcbd58/awscli-1.15.70-py2.py3-none-any.whl (1.3MB)\n","\u001b[K    100% |████████████████████████████████| 1.3MB 1.6MB/s \n","\u001b[?25hRequirement already satisfied: overrides in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.9)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from allennlp) (1.14.5)\n","Collecting flask==0.12.1 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/f4/43/fb2d5fb1d10e1d0402dd57836cf9a78b7f69c8b5f76a04b6e6113d0d7c5a/Flask-0.12.1-py2.py3-none-any.whl (82kB)\n","\u001b[K    100% |████████████████████████████████| 92kB 15.9MB/s \n","\u001b[?25hCollecting unidecode (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/59/ef/67085e30e8bbcdd76e2f0a4ad8151c13a2c5bce77c85f8cad6e1f16fb141/Unidecode-1.0.22-py2.py3-none-any.whl (235kB)\n","\u001b[K    100% |████████████████████████████████| 235kB 3.8MB/s \n","\u001b[?25hCollecting cffi==1.11.2 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/1e/b383fde1f0a14b6ef5a60f71797c778ea1ef8bb34b726cb57061c0542c58/cffi-1.11.2-cp36-cp36m-manylinux1_x86_64.whl (419kB)\n","\u001b[K    100% |████████████████████████████████| 430kB 3.4MB/s \n","\u001b[?25hCollecting psycopg2 (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/5e/d0/9e2b3ed43001ebed45caf56d5bb9d44ed3ebd68e12b87845bfa7bcd46250/psycopg2-2.7.5-cp36-cp36m-manylinux1_x86_64.whl (2.7MB)\n","\u001b[K    100% |████████████████████████████████| 2.7MB 3.4MB/s \n","\u001b[?25hRequirement already satisfied: spacy<2.1,>=2.0 in /usr/local/lib/python3.6/dist-packages (from allennlp) (2.0.12)\n","Collecting pytest (from allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d2/86/7b9513da923b94e48c2cf013ae4eae8184a36ebeb7fe27d386bc3db4f56f/pytest-3.7.1-py2.py3-none-any.whl (202kB)\n","\u001b[K    100% |████████████████████████████████| 204kB 19.4MB/s \n","\u001b[?25hRequirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from allennlp) (0.19.1)\n","Collecting responses>=0.7 (from allennlp)\n","  Downloading https://files.pythonhosted.org/packages/d9/a2/1cf64651ca0837ea627fbf0455231611bec85dbb7a9ffe761365950cabe5/responses-0.9.0-py2.py3-none-any.whl\n","Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from h5py->allennlp) (1.11.0)\n","Requirement already satisfied: protobuf>=0.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorboardX==1.2->allennlp) (3.6.0)\n","Requirement already satisfied: pyparsing>=2.0.3 in /usr/local/lib/python3.6/dist-packages (from pyhocon==0.3.35->allennlp) (2.2.0)\n"],"name":"stdout"},{"output_type":"stream","text":["Collecting greenlet>=0.4.10 (from gevent==1.2.2->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/de/7b/cb662640540725deb0627264f6b890ee2b7725848b8cbca49e27bf3273c6/greenlet-0.4.14-cp36-cp36m-manylinux1_x86_64.whl (41kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 1.7MB/s \n","\u001b[?25hRequirement already satisfied: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (1.22)\n","Requirement already satisfied: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2.6)\n","Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (3.0.4)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.18->allennlp) (2018.4.16)\n","Collecting sphinx>=1.2.3 (from numpydoc==0.8.0->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6d/62/e95cbd4966acaecb8805c321609ba986e47d806e5735967acfe2db3f7c26/Sphinx-1.7.6-py2.py3-none-any.whl (1.9MB)\n","\u001b[K    100% |████████████████████████████████| 1.9MB 2.8MB/s \n","\u001b[?25hRequirement already satisfied: Jinja2>=2.3 in /usr/local/lib/python3.6/dist-packages (from numpydoc==0.8.0->allennlp) (2.10)\n","Requirement already satisfied: PyYAML<=3.13,>=3.10 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.13)\n","Collecting docutils>=0.10 (from awscli>=1.11.91->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/fa/08e9e6e0e3cbd1d362c3bbee8d01d0aedb2155c4ac112b19ef3cae8eed8d/docutils-0.14-py3-none-any.whl (543kB)\n","\u001b[K    100% |████████████████████████████████| 552kB 3.2MB/s \n","\u001b[?25hCollecting botocore==1.10.69 (from awscli>=1.11.91->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/45/f3/0e5006bc6198213b853ca3975c537335a91716ffa1ed768e2b2ffe12d7ed/botocore-1.10.69-py2.py3-none-any.whl (4.4MB)\n","\u001b[K    100% |████████████████████████████████| 4.4MB 1.4MB/s \n","\u001b[?25hRequirement already satisfied: rsa<=3.5.0,>=3.1.2 in /usr/local/lib/python3.6/dist-packages (from awscli>=1.11.91->allennlp) (3.4.2)\n","Collecting colorama<=0.3.9,>=0.2.5 (from awscli>=1.11.91->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/db/c8/7dcf9dbcb22429512708fe3a547f8b6101c0d02137acbd892505aee57adf/colorama-0.3.9-py2.py3-none-any.whl\n","Collecting s3transfer<0.2.0,>=0.1.12 (from awscli>=1.11.91->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d7/14/2a0004d487464d120c9fb85313a75cd3d71a7506955be458eebfe19a6b1d/s3transfer-0.1.13-py2.py3-none-any.whl (59kB)\n","\u001b[K    100% |████████████████████████████████| 61kB 11.4MB/s \n","\u001b[?25hRequirement already satisfied: Werkzeug>=0.7 in /usr/local/lib/python3.6/dist-packages (from flask==0.12.1->allennlp) (0.14.1)\n","Collecting click>=2.0 (from flask==0.12.1->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/34/c1/8806f99713ddb993c5366c362b2f908f18269f8d792aff1abfd700775a77/click-6.7-py2.py3-none-any.whl (71kB)\n","\u001b[K    100% |████████████████████████████████| 71kB 10.8MB/s \n","\u001b[?25hCollecting itsdangerous>=0.21 (from flask==0.12.1->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/dc/b4/a60bcdba945c00f6d608d8975131ab3f25b22f2bcfe1dab221165194b2d4/itsdangerous-0.24.tar.gz (46kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 10.5MB/s \n","\u001b[?25hCollecting pycparser (from cffi==1.11.2->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/8c/2d/aad7f16146f4197a11f8e91fb81df177adcc2073d36a17b1491fd09df6ed/pycparser-2.18.tar.gz (245kB)\n","\u001b[K    100% |████████████████████████████████| 256kB 3.1MB/s \n","\u001b[?25hRequirement already satisfied: regex==2017.4.5 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (2017.4.5)\n","Requirement already satisfied: preshed<2.0.0,>=1.0.0 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.0.1)\n","Requirement already satisfied: ujson>=1.35 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.35)\n","Requirement already satisfied: dill<0.3,>=0.2 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.2.8.2)\n","Requirement already satisfied: murmurhash<0.29,>=0.28 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.28.0)\n","Requirement already satisfied: cymem<1.32,>=1.30 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (1.31.2)\n","Requirement already satisfied: plac<1.0.0,>=0.9.6 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (0.9.6)\n","Requirement already satisfied: thinc<6.11.0,>=6.10.3 in /usr/local/lib/python3.6/dist-packages (from spacy<2.1,>=2.0->allennlp) (6.10.3)\n","Collecting more-itertools>=4.0.0 (from pytest->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/79/b1/eace304ef66bd7d3d8b2f78cc374b73ca03bc53664d78151e9df3b3996cc/more_itertools-4.3.0-py3-none-any.whl (48kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 4.0MB/s \n","\u001b[?25hCollecting attrs>=17.4.0 (from pytest->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/41/59/cedf87e91ed541be7957c501a92102f9cc6363c623a7666d69d51c78ac5b/attrs-18.1.0-py2.py3-none-any.whl\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (39.1.0)\n","Collecting atomicwrites>=1.0 (from pytest->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/0a/e8/cd6375e7a59664eeea9e1c77a766eeac0fc3083bb958c2b41ec46b95f29c/atomicwrites-1.1.5-py2.py3-none-any.whl\n"],"name":"stdout"},{"output_type":"stream","text":["Requirement already satisfied: py>=1.5.0 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (1.5.4)\r\n","Requirement already satisfied: pluggy>=0.7 in /usr/local/lib/python3.6/dist-packages (from pytest->allennlp) (0.7.1)\n","Collecting cookies (from responses>=0.7->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6a/60/557f84aa2db629e5124aa05408b975b1b5d0e1cec16cde0bfa06aae097d3/cookies-2.2.1-py2.py3-none-any.whl (44kB)\n","\u001b[K    100% |████████████████████████████████| 51kB 13.9MB/s \n","\u001b[?25hCollecting alabaster<0.8,>=0.7 (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/6e/71/c3648cc2f675063dbe2d669004a59e4a5120172713a1de3c3b14144d4b31/alabaster-0.7.11-py2.py3-none-any.whl\n","Requirement already satisfied: Pygments>=2.0 in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (2.1.3)\n","Collecting imagesize (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/e9/79/31cc1c2e0daf575f8fd2b581e2975e6a6938bd439581f766b79c50479521/imagesize-1.0.0-py2.py3-none-any.whl\n","Collecting sphinxcontrib-websupport (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/52/69/3c2fbdc3702358c5b34ee25e387b24838597ef099761fc9a42c166796e8f/sphinxcontrib_websupport-1.1.0-py2.py3-none-any.whl\n","Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp) (17.1)\n","Collecting snowballstemmer>=1.1 (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/d4/6c/8a935e2c7b54a37714656d753e4187ee0631988184ed50c0cf6476858566/snowballstemmer-1.2.1-py2.py3-none-any.whl (64kB)\n","\u001b[K    100% |████████████████████████████████| 71kB 17.2MB/s \n","\u001b[?25hCollecting babel!=2.0,>=1.3 (from sphinx>=1.2.3->numpydoc==0.8.0->allennlp)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/b8/ad/c6f60602d3ee3d92fbed87675b6fb6a6f9a38c223343ababdb44ba201f10/Babel-2.6.0-py2.py3-none-any.whl (8.1MB)\n","\u001b[K    100% |████████████████████████████████| 8.1MB 1.7MB/s \n","\u001b[?25hRequirement already satisfied: MarkupSafe>=0.23 in /usr/local/lib/python3.6/dist-packages (from Jinja2>=2.3->numpydoc==0.8.0->allennlp) (1.0)\n","Requirement already satisfied: python-dateutil<3.0.0,>=2.1; python_version >= \"2.7\" in /usr/local/lib/python3.6/dist-packages (from botocore==1.10.69->awscli>=1.11.91->allennlp) (2.5.3)\n","Collecting jmespath<1.0.0,>=0.7.1 (from botocore==1.10.69->awscli>=1.11.91->allennlp)\n","  Downloading https://files.pythonhosted.org/packages/b7/31/05c8d001f7f87f0f07289a5fc0fc3832e9a57f2dbd4d3b0fee70e0d51365/jmespath-0.9.3-py2.py3-none-any.whl\n","Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<=3.5.0,>=3.1.2->awscli>=1.11.91->allennlp) (0.4.4)\n","Requirement already satisfied: wrapt<1.11.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy<2.1,>=2.0->allennlp) (1.10.11)\n","Requirement already satisfied: msgpack<1.0.0,>=0.5.6 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy<2.1,>=2.0->allennlp) (0.5.6)\n","Requirement already satisfied: msgpack-numpy<1.0.0,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy<2.1,>=2.0->allennlp) (0.4.3.1)\n","Requirement already satisfied: cytoolz<0.10,>=0.9.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy<2.1,>=2.0->allennlp) (0.9.0.1)\n","Requirement already satisfied: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy<2.1,>=2.0->allennlp) (0.9.0)\n","Building wheels for collected packages: pyhocon, numpydoc, itsdangerous, pycparser\n","  Running setup.py bdist_wheel for pyhocon ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/97/ea/cd/d2b70c1cfa9e6b1242709de5a64fd46be8982bc42be56d9390\n","  Running setup.py bdist_wheel for numpydoc ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ea/55/7f/3e25d754760ccd62d6796e5b2cfe25629346f52ea00753d549\n","  Running setup.py bdist_wheel for itsdangerous ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/2c/4a/61/5599631c1554768c6290b08c02c72d7317910374ca602ff1e5\n","  Running setup.py bdist_wheel for pycparser ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/c0/a1/27/5ba234bd77ea5a290cbf6d675259ec52293193467a12ef1f46\n","Successfully built pyhocon numpydoc itsdangerous pycparser\n"],"name":"stdout"},{"output_type":"stream","text":["Installing collected packages: tensorboardX, pyhocon, greenlet, gevent, flaky, pytz, alabaster, imagesize, docutils, sphinxcontrib-websupport, snowballstemmer, babel, sphinx, numpydoc, torch, conllu, editdistance, click, itsdangerous, flask, flask-cors, jmespath, botocore, colorama, s3transfer, awscli, unidecode, pycparser, cffi, psycopg2, more-itertools, attrs, atomicwrites, pytest, cookies, responses, allennlp\n","  Found existing installation: pytz 2018.5\n","    Uninstalling pytz-2018.5:\n","      Successfully uninstalled pytz-2018.5\n","Successfully installed alabaster-0.7.11 allennlp-0.5.1 atomicwrites-1.1.5 attrs-18.1.0 awscli-1.15.70 babel-2.6.0 botocore-1.10.69 cffi-1.11.2 click-6.7 colorama-0.3.9 conllu-0.10.6 cookies-2.2.1 docutils-0.14 editdistance-0.4 flaky-3.4.0 flask-0.12.1 flask-cors-3.0.3 gevent-1.2.2 greenlet-0.4.14 imagesize-1.0.0 itsdangerous-0.24 jmespath-0.9.3 more-itertools-4.3.0 numpydoc-0.8.0 psycopg2-2.7.5 pycparser-2.18 pyhocon-0.3.35 pytest-3.7.1 pytz-2017.3 responses-0.9.0 s3transfer-0.1.13 snowballstemmer-1.2.1 sphinx-1.7.6 sphinxcontrib-websupport-1.1.0 tensorboardX-1.2 torch-0.4.0 unidecode-1.0.22\n"],"name":"stdout"}]},{"metadata":{"id":"08BafNHdl5xg","colab_type":"text"},"cell_type":"markdown","source":["# Install other dependencies"]},{"metadata":{"id":"nbF__D10k0gO","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":1532},"outputId":"4741bf37-a8ea-4da5-ba95-025082909a04","executionInfo":{"status":"ok","timestamp":1533282065761,"user_tz":-540,"elapsed":23419,"user":{"displayName":"宮本圭一郎","photoUrl":"//lh5.googleusercontent.com/-5BLtx8oPSy8/AAAAAAAAAAI/AAAAAAAALtI/-tIwIsmAvCs/s50-c-k-no/photo.jpg","userId":"100227668169464343249"}}},"cell_type":"code","source":["!pip install overrides\n","!pip install tqdm\n","!pip install -U spacy\n","!python -m spacy download en"],"execution_count":2,"outputs":[{"output_type":"stream","text":["Collecting overrides\n","  Downloading https://files.pythonhosted.org/packages/de/55/3100c6d14c1ed177492fcf8f07c4a7d2d6c996c0a7fc6a9a0a41308e7eec/overrides-1.9.tar.gz\n","Building wheels for collected packages: overrides\n","  Running setup.py bdist_wheel for overrides ... \u001b[?25l-\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/8d/52/86/e5a83b1797e7d263b458d2334edd2704c78508b3eea9323718\n","Successfully built overrides\n","Installing collected packages: overrides\n","Successfully installed overrides-1.9\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.6/dist-packages (4.24.0)\n","Collecting spacy\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/24/de/ac14cd453c98656d6738a5669f96a4ac7f668493d5e6b78227ac933c5fd4/spacy-2.0.12.tar.gz (22.0MB)\n","\u001b[K    100% |████████████████████████████████| 22.0MB 1.7MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.7 in /usr/local/lib/python3.6/dist-packages (from spacy) (1.14.5)\n","Collecting murmurhash<0.29,>=0.28 (from spacy)\n","  Downloading https://files.pythonhosted.org/packages/5e/31/c8c1ecafa44db30579c8c457ac7a0f819e8b1dbc3e58308394fff5ff9ba7/murmurhash-0.28.0.tar.gz\n","Collecting cymem<1.32,>=1.30 (from spacy)\n","  Downloading https://files.pythonhosted.org/packages/f8/9e/273fbea507de99166c11cd0cb3fde1ac01b5bc724d9a407a2f927ede91a1/cymem-1.31.2.tar.gz\n","Collecting preshed<2.0.0,>=1.0.0 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/be/fc/09684555ce0ee7086675e6be698e4efeb6d9b315fd5aa96bed347572282b/preshed-1.0.1.tar.gz (112kB)\n","\u001b[K    100% |████████████████████████████████| 122kB 15.4MB/s \n","\u001b[?25hCollecting thinc<6.11.0,>=6.10.3 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/94/b1/47a88072d0a38b3594c0a638a62f9ef7c742b8b8a87f7b105f7ed720b14b/thinc-6.10.3.tar.gz (1.2MB)\n","\u001b[K    100% |████████████████████████████████| 1.2MB 10.9MB/s \n","\u001b[?25hCollecting plac<1.0.0,>=0.9.6 (from spacy)\n","  Downloading https://files.pythonhosted.org/packages/9e/9b/62c60d2f5bc135d2aa1d8c8a86aaf84edb719a59c7f11a4316259e61a298/plac-0.9.6-py2.py3-none-any.whl\n","Collecting ujson>=1.35 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/16/c4/79f3409bc710559015464e5f49b9879430d8f87498ecdc335899732e5377/ujson-1.35.tar.gz (192kB)\n","\u001b[K    100% |████████████████████████████████| 194kB 14.1MB/s \n","\u001b[?25hCollecting dill<0.3,>=0.2 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/6f/78/8b96476f4ae426db71c6e86a8e6a81407f015b34547e442291cd397b18f3/dill-0.2.8.2.tar.gz (150kB)\n","\u001b[K    100% |████████████████████████████████| 153kB 20.2MB/s \n","\u001b[?25hCollecting regex==2017.4.5 (from spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/62/c0c0d762ffd4ffaf39f372eb8561b8d491a11ace5a7884610424a8b40f95/regex-2017.04.05.tar.gz (601kB)\n","\u001b[K    100% |████████████████████████████████| 604kB 15.5MB/s \n","\u001b[?25hRequirement already satisfied, skipping upgrade: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.6/dist-packages (from spacy) (2.18.4)\n","Collecting msgpack<1.0.0,>=0.5.6 (from thinc<6.11.0,>=6.10.3->spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/22/4e/dcf124fd97e5f5611123d6ad9f40ffd6eb979d1efdc1049e28a795672fcd/msgpack-0.5.6-cp36-cp36m-manylinux1_x86_64.whl (315kB)\n","\u001b[K    100% |████████████████████████████████| 317kB 11.2MB/s \n","\u001b[?25hCollecting msgpack-numpy<1.0.0,>=0.4.1 (from thinc<6.11.0,>=6.10.3->spacy)\n","  Downloading https://files.pythonhosted.org/packages/84/09/fc890664a7a1dd0a88f46c93fb9340d0a27a69e82095a4a54aef2ed94a6d/msgpack_numpy-0.4.3.1-py2.py3-none-any.whl\n","Collecting cytoolz<0.10,>=0.9.0 (from thinc<6.11.0,>=6.10.3->spacy)\n","\u001b[?25l  Downloading https://files.pythonhosted.org/packages/36/f4/9728ba01ccb2f55df9a5af029b48ba0aaca1081bbd7823ea2ee223ba7a42/cytoolz-0.9.0.1.tar.gz (443kB)\n","\u001b[K    100% |████████████████████████████████| 450kB 12.4MB/s \n","\u001b[?25hCollecting wrapt<1.11.0,>=1.10.0 (from thinc<6.11.0,>=6.10.3->spacy)\n","  Downloading https://files.pythonhosted.org/packages/a0/47/66897906448185fcb77fc3c2b1bc20ed0ecca81a0f2f88eda3fc5a34fc3d/wrapt-1.10.11.tar.gz\n","Requirement already satisfied, skipping upgrade: tqdm<5.0.0,>=4.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (4.24.0)\n","Requirement already satisfied, skipping upgrade: six<2.0.0,>=1.10.0 in /usr/local/lib/python3.6/dist-packages (from thinc<6.11.0,>=6.10.3->spacy) (1.11.0)\n","Requirement already satisfied, skipping upgrade: urllib3<1.23,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (1.22)\n","Requirement already satisfied, skipping upgrade: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2018.4.16)\n","Requirement already satisfied, skipping upgrade: idna<2.7,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.6)\n","Requirement already satisfied, skipping upgrade: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.0.4)\n","Requirement already satisfied, skipping upgrade: toolz>=0.8.0 in /usr/local/lib/python3.6/dist-packages (from cytoolz<0.10,>=0.9.0->thinc<6.11.0,>=6.10.3->spacy) (0.9.0)\n","Building wheels for collected packages: spacy, murmurhash, cymem, preshed, thinc, ujson, dill, regex, cytoolz, wrapt\n"],"name":"stdout"},{"output_type":"stream","text":["  Running setup.py bdist_wheel for spacy ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/60/0b/bb/7c2e28db574dbb2358176934eddd32a1c5f838ba0bc23eaaab\n","  Running setup.py bdist_wheel for murmurhash ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/b8/94/a4/f69f8664cdc1098603df44771b7fec5fd1b3d8364cdd83f512\n","  Running setup.py bdist_wheel for cymem ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/55/8d/4a/f6328252aa2aaec0b1cb906fd96a1566d77f0f67701071ad13\n","  Running setup.py bdist_wheel for preshed ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/ca/e5/8b/73706d7232da301838e0bc564367a2f7b2fc8f834228fc8a4b\n","  Running setup.py bdist_wheel for thinc ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/db/bc/e1/9b321b6b203288cf636a56e668ed5700076af4ed66062278ca\n","  Running setup.py bdist_wheel for ujson ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/28/77/e4/0311145b9c2e2f01470e744855131f9e34d6919687550f87d1\n","  Running setup.py bdist_wheel for dill ... \u001b[?25l-\b \b\\\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/e2/5d/17/f87cb7751896ac629b435a8696f83ee75b11029f5d6f6bda72\n","  Running setup.py bdist_wheel for regex ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-"],"name":"stdout"},{"output_type":"stream","text":["\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/75/07/38/3c16b529d50cb4e0cd3dbc7b75cece8a09c132692c74450b01\n","  Running setup.py bdist_wheel for cytoolz ... \u001b[?25l-\b \b\\\b \b|\b \b/\b \b-\b \b\\\b \b|\b \b/\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/88/f3/11/9817b001e59ab04889e8cffcbd9087e2e2155b9ebecfc8dd38\n","  Running setup.py bdist_wheel for wrapt ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25h  Stored in directory: /content/.cache/pip/wheels/48/5d/04/22361a593e70d23b1f7746d932802efe1f0e523376a74f321e\n","Successfully built spacy murmurhash cymem preshed thinc ujson dill regex cytoolz wrapt\n","Installing collected packages: murmurhash, cymem, preshed, msgpack, msgpack-numpy, cytoolz, wrapt, plac, dill, thinc, ujson, regex, spacy\n","Successfully installed cymem-1.31.2 cytoolz-0.9.0.1 dill-0.2.8.2 msgpack-0.5.6 msgpack-numpy-0.4.3.1 murmurhash-0.28.0 plac-0.9.6 preshed-1.0.1 regex-2017.4.5 spacy-2.0.12 thinc-6.10.3 ujson-1.35 wrapt-1.10.11\n","Collecting en_core_web_sm==2.0.0 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz#egg=en_core_web_sm==2.0.0\n","\u001b[?25l  Downloading https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.0.0/en_core_web_sm-2.0.0.tar.gz (37.4MB)\n","\u001b[K    100% |████████████████████████████████| 37.4MB 18.0MB/s \n","\u001b[?25hInstalling collected packages: en-core-web-sm\n","  Running setup.py install for en-core-web-sm ... \u001b[?25l-\b \b\\\b \b|\b \bdone\n","\u001b[?25hSuccessfully installed en-core-web-sm-2.0.0\n","\n","\u001b[93m    Linking successful\u001b[0m\n","    /usr/local/lib/python3.6/dist-packages/en_core_web_sm -->\n","    /usr/local/lib/python3.6/dist-packages/spacy/data/en\n","\n","    You can now load the model via spacy.load('en')\n","\n"],"name":"stdout"}]},{"metadata":{"id":"9VJg1JfbwZh9","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":34},"outputId":"3c4441da-1e77-452e-8d08-ed31d4537f2a","executionInfo":{"status":"ok","timestamp":1533282488196,"user_tz":-540,"elapsed":840,"user":{"displayName":"宮本圭一郎","photoUrl":"//lh5.googleusercontent.com/-5BLtx8oPSy8/AAAAAAAAAAI/AAAAAAAALtI/-tIwIsmAvCs/s50-c-k-no/photo.jpg","userId":"100227668169464343249"}}},"cell_type":"code","source":["import torch\n","torch.cuda.is_available()"],"execution_count":1,"outputs":[{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{"tags":[]},"execution_count":1}]},{"metadata":{"id":"bS7UCu_lnAyP","colab_type":"text"},"cell_type":"markdown","source":["# 処理"]},{"metadata":{"id":"FyW80BcGniPA","colab_type":"text"},"cell_type":"markdown","source":["NLPタスクを扱うには異なるタイプのニューラルネットワークセルが必要。\n","\n","- RNN\n","- LSTM\n","- etc...\n"]},{"metadata":{"id":"eT_vO1KZyhmZ","colab_type":"text"},"cell_type":"markdown","source":["## 1-データ入力\n","\n","DatasetReaderに入力データセットと読み込み方法をJSONで指定。この場合20個のnewgroupsデータセットを使用。\n"]},{"metadata":{"id":"Y7qaU-eKlxmP","colab_type":"code","colab":{}},"cell_type":"code","source":["from typing import Dict\n","import json\n","import logging\n","\n","from overrides import overrides\n","\n","import tqdm\n","\n","from allennlp.common import Params\n","from allennlp.common.file_utils import cached_path\n","from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n","from allennlp.data.fields import LabelField, TextField\n","from allennlp.data.instance import Instance\n","from allennlp.data.tokenizers import Tokenizer, WordTokenizer\n","from allennlp.data.token_indexers import TokenIndexer, SingleIdTokenIndexer\n","\n","from sklearn.datasets import fetch_20newsgroups\n","\n","logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n","\n","\n","@DatasetReader.register(\"20newsgroups\")\n","class NewsgroupsDatasetReader(DatasetReader):\n","    \"\"\"\n","    Reads a JSON-lines file containing papers from the Semantic Scholar database, and creates a\n","    dataset suitable for document classification using these papers.\n","\n","    Expected format for each input line: {\"paperAbstract\": \"text\", \"title\": \"text\", \"venue\": \"text\"}\n","\n","    The JSON could have other fields, too, but they are ignored.\n","\n","    The output of ``read`` is a list of ``Instance`` s with the fields:\n","        title: ``TextField``\n","        abstract: ``TextField``\n","        label: ``LabelField``\n","\n","    where the ``label`` is derived from the venue of the paper.\n","\n","    Parameters\n","    ----------\n","    tokenizer : ``Tokenizer``, optional\n","        Tokenizer to use to split the title and abstrct into words or other kinds of tokens.\n","        Defaults to ``WordTokenizer()``.\n","    token_indexers : ``Dict[str, TokenIndexer]``, optional\n","        Indexers used to define input token representations. Defaults to ``{\"tokens\":\n","        SingleIdTokenIndexer()}``.\n","    \"\"\"\n","    def __init__(self,\n","                 tokenizer: Tokenizer = None,\n","                 token_indexers: Dict[str, TokenIndexer] = None) -> None:\n","        self._tokenizer = tokenizer or WordTokenizer()\n","        self._token_indexers = token_indexers or {\"tokens\": SingleIdTokenIndexer()}\n","\n","    @overrides\n","    def _read(self, file_path):\n","        instances = []\n","        if file_path == \"train\":\n","            logger.info(\"Reading instances from: %s\", file_path)\n","            categories = [\"comp.graphics\",\"sci.space\",\"rec.sport.baseball\"]\n","            newsgroups_data = fetch_20newsgroups(subset='train',categories=categories)\n","            \n","        elif file_path == \"test\":\n","            logger.info(\"Reading instances from: %s\", file_path)\n","            categories = [\"comp.graphics\",\"sci.space\",\"rec.sport.baseball\"]\n","            newsgroups_data = fetch_20newsgroups(subset='test',categories=categories)\n","            \n","        else:\n","            raise ConfigurationError(\"Path string not specified in read method\")\n","            \n","        for i,text in enumerate(newsgroups_data.data):\n","            if file_path == \"validate\":\n","                if i == 400:\n","                    break\n","            text = newsgroups_data.data[i]\n","            target = newsgroups_data.target[i]\n","            yield self.text_to_instance(text, target)\n","\n","    @overrides\n","    def text_to_instance(self, text: str, target: str = None) -> Instance:  # type: ignore\n","        # pylint: disable=arguments-differ\n","        tokenized_text = self._tokenizer.tokenize(text)\n","        text_field = TextField(tokenized_text, self._token_indexers)\n","        fields = {'text': text_field}\n","        if target is not None:\n","            fields['label'] = LabelField(int(target),skip_indexing=True)\n","        return Instance(fields)\n","\n","    @classmethod\n","    def from_params(cls, params: Params) -> 'NewsgroupsDatasetReader':\n","        tokenizer = Tokenizer.from_params(params.pop('tokenizer', {}))\n","        token_indexers = TokenIndexer.dict_from_params(params.pop('token_indexers', {}))\n","        params.assert_empty(cls.__name__)\n","        return cls(tokenizer=tokenizer, token_indexers=token_indexers)\n","\n","\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Faz0bxBdzXCf","colab_type":"text"},"cell_type":"markdown","source":["## 2-モデル\n","\n"]},{"metadata":{"id":"Ikc4ybzMmpcM","colab_type":"code","colab":{}},"cell_type":"code","source":["from typing import Dict, Optional\n","\n","import numpy\n","from overrides import overrides\n","import torch\n","import torch.nn.functional as F\n","\n","from allennlp.common import Params\n","from allennlp.common.checks import ConfigurationError\n","from allennlp.data import Vocabulary\n","from allennlp.modules import FeedForward, Seq2VecEncoder, TextFieldEmbedder\n","from allennlp.models.model import Model\n","from allennlp.nn import InitializerApplicator, RegularizerApplicator\n","from allennlp.nn import util\n","from allennlp.training.metrics import CategoricalAccuracy\n","\n","\n","@Model.register(\"20newsgroups_classifier\")\n","class Fetch20NewsgroupsClassifier(Model):\n","    \"\"\"\n","    This ``Model`` performs text classification for a newsgroup text.  We assume we're given a\n","    text and we predict some output label.\n","    The basic model structure: we'll embed the text and encode it with\n","    a Seq2VecEncoder, getting a single vector representing the content.  We'll then\n","    the result through a feedforward network, the output of\n","    which we'll use as our scores for each label.\n","    Parameters\n","    ----------\n","    vocab : ``Vocabulary``, required\n","        A Vocabulary, required in order to compute sizes for input/output projections.\n","    model_text_field_embedder : ``TextFieldEmbedder``, required\n","        Used to embed the ``tokens`` ``TextField`` we get as input to the model.\n","    internal_text_encoder : ``Seq2VecEncoder``\n","        The encoder that we will use to convert the input text to a vector.\n","    classifier_feedforward : ``FeedForward``\n","    initializer : ``InitializerApplicator``, optional (default=``InitializerApplicator()``)\n","        Used to initialize the model parameters.\n","    regularizer : ``RegularizerApplicator``, optional (default=``None``)\n","        If provided, will be used to calculate the regularization penalty during training.\n","    \"\"\"\n","    def __init__(self, vocab: Vocabulary,\n","                 model_text_field_embedder: TextFieldEmbedder,\n","                 internal_text_encoder: Seq2VecEncoder,\n","                 classifier_feedforward: FeedForward,\n","                 initializer: InitializerApplicator = InitializerApplicator(),\n","                 regularizer: Optional[RegularizerApplicator] = None) -> None:\n","        super(Fetch20NewsgroupsClassifier, self).__init__(vocab, regularizer)\n","\n","        self.model_text_field_embedder = model_text_field_embedder\n","        self.num_classes = self.vocab.get_vocab_size(\"labels\")\n","        self.internal_text_encoder = internal_text_encoder\n","        self.classifier_feedforward = classifier_feedforward\n","\n","        if model_text_field_embedder.get_output_dim() != internal_text_encoder.get_input_dim():\n","            raise ConfigurationError(\"The output dimension of the model_text_field_embedder must match the \"\n","                                     \"input dimension of the title_encoder. Found {} and {}, \"\n","                                     \"respectively.\".format(model_text_field_embedder.get_output_dim(),\n","                                                            internal_text_encoder.get_input_dim()))\n","        self.metrics = {\n","                \"accuracy\": CategoricalAccuracy(),\n","                \"accuracy3\": CategoricalAccuracy(top_k=3)\n","        }\n","        self.loss = torch.nn.CrossEntropyLoss()\n","\n","        initializer(self)\n","\n","    @overrides\n","    def forward(self,  # type: ignore\n","                text: Dict[str, torch.LongTensor],\n","                label: torch.LongTensor = None) -> Dict[str, torch.Tensor]:\n","        # pylint: disable=arguments-differ\n","        \"\"\"\n","        Parameters\n","        ----------\n","        input_text : Dict[str, Variable], required\n","            The output of ``TextField.as_array()``.\n","        label : Variable, optional (default = None)\n","            A variable representing the label for each instance in the batch.\n","        Returns\n","        -------\n","        An output dictionary consisting of:\n","        class_probabilities : torch.FloatTensor\n","            A tensor of shape ``(batch_size, num_classes)`` representing a distribution over the\n","            label classes for each instance.\n","        loss : torch.FloatTensor, optional\n","            A scalar loss to be optimised.\n","        \"\"\"\n","        embedded_text = self.model_text_field_embedder(text)\n","        text_mask = util.get_text_field_mask(text)\n","        encoded_text = self.internal_text_encoder(embedded_text, text_mask)\n","\n","        logits = self.classifier_feedforward(encoded_text)\n","        output_dict = {'logits': logits}\n","        if label is not None:\n","            loss = self.loss(logits, label.squeeze(-1))\n","            for metric in self.metrics.values():\n","                metric(logits, label.squeeze(-1))\n","            output_dict[\"loss\"] = loss\n","\n","        return output_dict\n","\n","    @overrides\n","    def decode(self, output_dict: Dict[str, torch.Tensor]) -> Dict[str, torch.Tensor]:\n","        \"\"\"\n","        Does a simple argmax over the class probabilities, converts indices to string labels, and\n","        adds a ``\"label\"`` key to the dictionary with the result.\n","        \"\"\"\n","        class_probabilities = F.softmax(output_dict['logits'])\n","        output_dict['class_probabilities'] = class_probabilities\n","\n","        predictions = class_probabilities.cpu().data.numpy()\n","        argmax_indices = numpy.argmax(predictions, axis=-1)\n","        labels = [self.vocab.get_token_from_index(x, namespace=\"labels\")\n","                  for x in argmax_indices]\n","        output_dict['label'] = labels\n","        return output_dict\n","\n","    @overrides\n","    def get_metrics(self, reset: bool = False) -> Dict[str, float]:\n","        return {metric_name: metric.get_metric(reset) for metric_name, metric in self.metrics.items()}\n","\n","    @classmethod\n","    def from_params(cls, vocab: Vocabulary, params: Params) -> 'Fetch20NewsgroupsClassifier':\n","        embedder_params = params.pop(\"model_text_field_embedder\")\n","        model_text_field_embedder = TextFieldEmbedder.from_params(vocab, embedder_params)\n","        internal_text_encoder = Seq2VecEncoder.from_params(params.pop(\"internal_text_encoder\"))\n","        classifier_feedforward = FeedForward.from_params(params.pop(\"classifier_feedforward\"))\n","\n","        initializer = InitializerApplicator.from_params(params.pop('initializer', []))\n","        regularizer = RegularizerApplicator.from_params(params.pop('regularizer', []))\n","\n","        return cls(vocab=vocab,\n","                   model_text_field_embedder=model_text_field_embedder,\n","                   internal_text_encoder=internal_text_encoder,\n","                   classifier_feedforward=classifier_feedforward,\n","                   initializer=initializer,\n","                   regularizer=regularizer)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"1yC0Qqcf0Y3Q","colab_type":"text"},"cell_type":"markdown","source":["## 3 - データイテレータ"]},{"metadata":{"id":"fcf2NcTvmr2K","colab_type":"code","colab":{}},"cell_type":"code","source":["from typing import Dict, Iterable\n","import argparse\n","import json\n","import logging\n","import os\n","import sys\n","from copy import deepcopy\n","\n","from allennlp.commands.evaluate import evaluate\n","from allennlp.commands.subcommand import Subcommand\n","from allennlp.common.checks import ConfigurationError\n","from allennlp.common import Params, TeeLogger, Tqdm\n","from allennlp.common.util import prepare_environment\n","from allennlp.data import Vocabulary\n","from allennlp.data.instance import Instance\n","from allennlp.data.dataset_readers.dataset_reader import DatasetReader\n","from allennlp.data.iterators.data_iterator import DataIterator\n","from allennlp.models.archival import archive_model, CONFIG_NAME\n","from allennlp.models.model import Model\n","from allennlp.training.trainer import Trainer\n","\n","logger = logging.getLogger(__name__)  # pylint: disable=invalid-name\n","\n","\n","class Train(Subcommand):\n","    def add_subparser(self, name: str, parser: argparse._SubParsersAction) -> argparse.ArgumentParser:\n","        # pylint: disable=protected-access\n","        description = '''Train the specified model on the specified dataset.'''\n","        subparser = parser.add_parser(name, description=description, help='Train a model')\n","\n","        subparser.add_argument('param_path',\n","                               type=str,\n","                               help='path to parameter file describing the model to be trained')\n","\n","        subparser.add_argument('-s', '--serialization-dir',\n","                               required=True,\n","                               type=str,\n","                               help='directory in which to save the model and its logs')\n","\n","        subparser.add_argument('-r', '--recover',\n","                               action='store_true',\n","                               default=False,\n","                               help='recover training from the state in serialization_dir')\n","\n","        subparser.add_argument('-o', '--overrides',\n","                               type=str,\n","                               default=\"\",\n","                               help='a HOCON structure used to override the experiment configuration')\n","\n","        subparser.add_argument('--file-friendly-logging',\n","                               action='store_true',\n","                               default=False,\n","                               help='outputs tqdm status on separate lines and slows tqdm refresh rate')\n","\n","        subparser.set_defaults(func=train_model_from_args)\n","\n","        return subparser\n","\n","def train_model_from_args(args: argparse.Namespace):\n","    \"\"\"\n","    Just converts from an ``argparse.Namespace`` object to string paths.\n","    \"\"\"\n","    train_model_from_file(args.param_path,\n","                          args.serialization_dir,\n","                          args.overrides,\n","                          args.file_friendly_logging,\n","                          args.recover)\n","\n","\n","def train_model_from_file(parameter_filename: str,\n","                          serialization_dir: str,\n","                          overrides: str = \"\",\n","                          file_friendly_logging: bool = False,\n","                          recover: bool = False) -> Model:\n","    \"\"\"\n","    A wrapper around :func:`train_model` which loads the params from a file.\n","    Parameters\n","    ----------\n","    param_path : ``str``\n","        A json parameter file specifying an AllenNLP experiment.\n","    serialization_dir : ``str``\n","        The directory in which to save results and logs. We just pass this along to\n","        :func:`train_model`.\n","    overrides : ``str``\n","        A HOCON string that we will use to override values in the input parameter file.\n","    file_friendly_logging : ``bool``, optional (default=False)\n","        If ``True``, we make our output more friendly to saved model files.  We just pass this\n","        along to :func:`train_model`.\n","    recover : ``bool`, optional (default=False)\n","        If ``True``, we will try to recover a training run from an existing serialization\n","        directory.  This is only intended for use when something actually crashed during the middle\n","        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n","    \"\"\"\n","    # Load the experiment config from a file and pass it to ``train_model``.\n","    params = Params.from_file(parameter_filename, overrides)\n","    return train_model(params, serialization_dir, file_friendly_logging, recover)\n","\n","\n","def datasets_from_params(params: Params) -> Dict[str, Iterable[Instance]]:\n","    \"\"\"\n","    Load all the datasets specified by the config.\n","    \"\"\"\n","    dataset_reader = DatasetReader.from_params(params.pop('dataset_reader'))\n","    validation_dataset_reader_params = params.pop(\"validation_dataset_reader\", None)\n","\n","    validation_and_test_dataset_reader: DatasetReader = dataset_reader\n","    if validation_dataset_reader_params is not None:\n","        logger.info(\"Using a separate dataset reader to load validation and test data.\")\n","        validation_and_test_dataset_reader = DatasetReader.from_params(validation_dataset_reader_params)\n","\n","    train_data_path = params.pop('train_data_path')\n","    logger.info(\"Reading training data from %s\", train_data_path)\n","    train_data = dataset_reader.read(train_data_path)\n","\n","    datasets: Dict[str, Iterable[Instance]] = {\"train\": train_data}\n","\n","    validation_data_path = params.pop('validation_data_path', None)\n","    if validation_data_path is not None:\n","        logger.info(\"Reading validation data from %s\", validation_data_path)\n","        validation_data = validation_and_test_dataset_reader.read(validation_data_path)\n","        datasets[\"validation\"] = validation_data\n","\n","    test_data_path = params.pop(\"test_data_path\", None)\n","    if test_data_path is not None:\n","        logger.info(\"Reading test data from %s\", test_data_path)\n","        test_data = validation_and_test_dataset_reader.read(test_data_path)\n","        datasets[\"test\"] = test_data\n","\n","    return datasets\n","\n","def create_serialization_dir(params: Params, serialization_dir: str, recover: bool) -> None:\n","    \"\"\"\n","    This function creates the serialization directory if it doesn't exist.  If it already exists,\n","    then it verifies that we're recovering from a training with an identical configuration.\n","    Parameters\n","    ----------\n","    params: ``Params``\n","        A parameter object specifying an AllenNLP Experiment.\n","    serialization_dir: ``str``\n","        The directory in which to save results and logs.\n","    recover: ``bool``\n","        If ``True``, we will try to recover from an existing serialization directory, and crash if\n","        the directory doesn't exist, or doesn't match the configuration we're given.\n","    \"\"\"\n","    if os.path.exists(serialization_dir):\n","        if serialization_dir == '/output':\n","            # Special-casing the beaker output directory, which will already exist when training\n","            # starts.\n","            return\n","        if not recover:\n","            raise ConfigurationError(f\"Serialization directory ({serialization_dir}) already exists.  \"\n","                                     f\"Specify --recover to recover training from existing output.\")\n","\n","        logger.info(f\"Recovering from prior training at {serialization_dir}.\")\n","\n","        recovered_config_file = os.path.join(serialization_dir, CONFIG_NAME)\n","        if not os.path.exists(recovered_config_file):\n","            raise ConfigurationError(\"The serialization directory already exists but doesn't \"\n","                                     \"contain a config.json. You probably gave the wrong directory.\")\n","        else:\n","            loaded_params = Params.from_file(recovered_config_file)\n","\n","            # Check whether any of the training configuration differs from the configuration we are\n","            # resuming.  If so, warn the user that training may fail.\n","            fail = False\n","            flat_params = params.as_flat_dict()\n","            flat_loaded = loaded_params.as_flat_dict()\n","            for key in flat_params.keys() - flat_loaded.keys():\n","                logger.error(f\"Key '{key}' found in training configuration but not in the serialization \"\n","                             f\"directory we're recovering from.\")\n","                fail = True\n","            for key in flat_loaded.keys() - flat_params.keys():\n","                logger.error(f\"Key '{key}' found in the serialization directory we're recovering from \"\n","                             f\"but not in the training config.\")\n","                fail = True\n","            for key in flat_params.keys():\n","                if flat_params.get(key, None) != flat_loaded.get(key, None):\n","                    logger.error(f\"Value for '{key}' in training configuration does not match that the value in \"\n","                                 f\"the serialization directory we're recovering from: \"\n","                                 f\"{flat_params[key]} != {flat_loaded[key]}\")\n","                    fail = True\n","            if fail:\n","                raise ConfigurationError(\"Training configuration does not match the configuration we're \"\n","                                         \"recovering from.\")\n","    else:\n","        if recover:\n","            raise ConfigurationError(f\"--recover specified but serialization_dir ({serialization_dir}) \"\n","                                     \"does not exist.  There is nothing to recover from.\")\n","        os.makedirs(serialization_dir)\n","\n","\n","def train_model(params: Params,\n","                serialization_dir: str,\n","                file_friendly_logging: bool = False,\n","                recover: bool = False) -> Model:\n","    \"\"\"\n","    Trains the model specified in the given :class:`Params` object, using the data and training\n","    parameters also specified in that object, and saves the results in ``serialization_dir``.\n","    Parameters\n","    ----------\n","    params : ``Params``\n","        A parameter object specifying an AllenNLP Experiment.\n","    serialization_dir : ``str``\n","        The directory in which to save results and logs.\n","    file_friendly_logging : ``bool``, optional (default=False)\n","        If ``True``, we add newlines to tqdm output, even on an interactive terminal, and we slow\n","        down tqdm's output to only once every 10 seconds.\n","    recover : ``bool`, optional (default=False)\n","        If ``True``, we will try to recover a training run from an existing serialization\n","        directory.  This is only intended for use when something actually crashed during the middle\n","        of a run.  For continuing training a model on new data, see the ``fine-tune`` command.\n","    \"\"\"\n","    prepare_environment(params)\n","\n","    create_serialization_dir(params, serialization_dir, recover)\n","\n","    # TODO(mattg): pull this block out into a separate function (maybe just add this to\n","    # `prepare_environment`?)\n","    Tqdm.set_slower_interval(file_friendly_logging)\n","    sys.stdout = TeeLogger(os.path.join(serialization_dir, \"stdout.log\"), # type: ignore\n","                           sys.stdout,\n","                           file_friendly_logging)\n","    sys.stderr = TeeLogger(os.path.join(serialization_dir, \"stderr.log\"), # type: ignore\n","                           sys.stderr,\n","                           file_friendly_logging)\n","    handler = logging.FileHandler(os.path.join(serialization_dir, \"python_logging.log\"))\n","    handler.setLevel(logging.INFO)\n","    handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(name)s - %(message)s'))\n","    logging.getLogger().addHandler(handler)\n","\n","    serialization_params = deepcopy(params).as_dict(quiet=True)\n","    with open(os.path.join(serialization_dir, CONFIG_NAME), \"w\") as param_file:\n","        json.dump(serialization_params, param_file, indent=4)\n","\n","    all_datasets = datasets_from_params(params)\n","    datasets_for_vocab_creation = set(params.pop(\"datasets_for_vocab_creation\", all_datasets))\n","\n","    for dataset in datasets_for_vocab_creation:\n","        if dataset not in all_datasets:\n","            raise ConfigurationError(f\"invalid 'dataset_for_vocab_creation' {dataset}\")\n","\n","    logger.info(\"Creating a vocabulary using %s data.\", \", \".join(datasets_for_vocab_creation))\n","    vocab = Vocabulary.from_params(params.pop(\"vocabulary\", {}),\n","                                   (instance for key, dataset in all_datasets.items()\n","                                    for instance in dataset\n","                                    if key in datasets_for_vocab_creation))\n","    vocab.save_to_files(os.path.join(serialization_dir, \"vocabulary\"))\n","\n","    model = Model.from_params(vocab, params.pop('model'))\n","    iterator = DataIterator.from_params(params.pop(\"iterator\"))\n","    iterator.index_with(vocab)\n","\n","    train_data = all_datasets['train']\n","    validation_data = all_datasets.get('validation')\n","    test_data = all_datasets.get('test')\n","\n","    trainer_params = params.pop(\"trainer\")\n","    trainer = Trainer.from_params(model,\n","                                  serialization_dir,\n","                                  iterator,\n","                                  train_data,\n","                                  validation_data,\n","                                  trainer_params)\n","\n","    evaluate_on_test = params.pop_bool(\"evaluate_on_test\", False)\n","    params.assert_empty('base train command')\n","    metrics = trainer.train()\n","\n","    # Now tar up results\n","    archive_model(serialization_dir, files_to_archive=params.files_to_archive)\n","\n","    if test_data and evaluate_on_test:\n","        test_metrics = evaluate(model, test_data, iterator, cuda_device=trainer._cuda_devices[0])  # pylint: disable=protected-access\n","        for key, value in test_metrics.items():\n","            metrics[\"test_\" + key] = value\n","\n","    elif test_data:\n","        logger.info(\"To evaluate on the test set after training, pass the \"\n","                    \"'evaluate_on_test' flag, or use the 'allennlp evaluate' command.\")\n","\n","    metrics_json = json.dumps(metrics, indent=2)\n","    with open(os.path.join(serialization_dir, \"metrics.json\"), \"w\") as metrics_file:\n","        metrics_file.write(metrics_json)\n","    logger.info(\"Metrics: %s\", metrics_json)\n","\n","    return model"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zAS0l2lemuF5","colab_type":"code","colab":{}},"cell_type":"code","source":["experiment_parameters = 'https://raw.githubusercontent.com/dmesquita/easy-deep-learning-with-AllenNLP/master/experiments/newsgroups_with_cuda.json'"],"execution_count":0,"outputs":[]},{"metadata":{"id":"bSOL0AMNwqpn","colab_type":"code","colab":{}},"cell_type":"code","source":["!ls /temp_dir\n","# !rm -R /temp_dir"],"execution_count":0,"outputs":[]},{"metadata":{"id":"dS0Ee_37mv_B","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":961},"outputId":"8dde0f09-effe-49cf-ac59-82681fba1931","executionInfo":{"status":"ok","timestamp":1533282800486,"user_tz":-540,"elapsed":253305,"user":{"displayName":"宮本圭一郎","photoUrl":"//lh5.googleusercontent.com/-5BLtx8oPSy8/AAAAAAAAAAI/AAAAAAAALtI/-tIwIsmAvCs/s50-c-k-no/photo.jpg","userId":"100227668169464343249"}}},"cell_type":"code","source":["train_model_from_file(experiment_parameters,\"/temp_dir\") "],"execution_count":9,"outputs":[{"output_type":"stream","text":["1774it [00:19, 92.61it/s]\n","1180it [00:14, 83.84it/s]\n","2954it [00:02, 1173.64it/s]\n","/usr/local/lib/python3.6/dist-packages/torch/nn/modules/rnn.py:38: UserWarning: dropout option adds dropout after all but last recurrent layer, so non-zero dropout expects num_layers greater than 1, but got dropout=0.2 and num_layers=1\n","  \"num_layers={}\".format(dropout, num_layers))\n","accuracy: 0.3444, accuracy3: 0.9656, loss: 1.4357 ||: 100%|##########| 28/28 [00:08<00:00,  3.48it/s]\n","accuracy: 0.3867, accuracy3: 1.0000, loss: 1.0985 ||: 100%|##########| 28/28 [00:06<00:00,  4.47it/s]\n","accuracy: 0.4735, accuracy3: 1.0000, loss: 1.0242 ||: 100%|##########| 28/28 [00:06<00:00,  4.50it/s]\n","accuracy: 0.5366, accuracy3: 1.0000, loss: 0.9449 ||: 100%|##########| 28/28 [00:06<00:00,  4.47it/s]\n","accuracy: 0.5970, accuracy3: 1.0000, loss: 0.8559 ||: 100%|##########| 28/28 [00:06<00:00,  4.44it/s]\n","accuracy: 0.6381, accuracy3: 1.0000, loss: 0.8178 ||: 100%|##########| 28/28 [00:06<00:00,  4.40it/s]\n","  0%|          | 0/28 [00:00<?, ?it/s]"],"name":"stderr"},{"output_type":"stream","text":["accuracy: 0.6815, accuracy3: 1.0000, loss: 0.7429 ||: 100%|##########| 28/28 [00:06<00:00,  4.42it/s]\n","accuracy: 0.7638, accuracy3: 1.0000, loss: 0.5888 ||: 100%|##########| 28/28 [00:06<00:00,  4.38it/s]\n","accuracy: 0.8269, accuracy3: 1.0000, loss: 0.4613 ||: 100%|##########| 28/28 [00:06<00:00,  4.49it/s]\n","accuracy: 0.8534, accuracy3: 1.0000, loss: 0.3888 ||: 100%|##########| 28/28 [00:06<00:00,  4.53it/s]\n","accuracy: 0.9025, accuracy3: 1.0000, loss: 0.2800 ||: 100%|##########| 28/28 [00:06<00:00,  4.39it/s]\n","accuracy: 0.9205, accuracy3: 1.0000, loss: 0.2250 ||: 100%|##########| 28/28 [00:06<00:00,  4.38it/s]\n","accuracy: 0.9335, accuracy3: 1.0000, loss: 0.1842 ||: 100%|##########| 28/28 [00:06<00:00,  4.40it/s]\n","accuracy: 0.9515, accuracy3: 1.0000, loss: 0.1471 ||: 100%|##########| 28/28 [00:06<00:00,  4.41it/s]\n","accuracy: 0.9718, accuracy3: 1.0000, loss: 0.1006 ||: 100%|##########| 28/28 [00:06<00:00,  4.37it/s]\n","accuracy: 0.9516, accuracy3: 1.0000, loss: 0.1441 ||:  36%|###5      | 10/28 [00:01<00:02,  7.50it/s]"],"name":"stderr"},{"output_type":"stream","text":["accuracy: 0.9600, accuracy3: 1.0000, loss: 0.1180 ||: 100%|##########| 28/28 [00:06<00:00,  4.39it/s]\n","accuracy: 0.9825, accuracy3: 1.0000, loss: 0.0632 ||: 100%|##########| 28/28 [00:06<00:00,  4.39it/s]\n","accuracy: 0.9853, accuracy3: 1.0000, loss: 0.0522 ||: 100%|##########| 28/28 [00:06<00:00,  4.44it/s]\n","accuracy: 0.9944, accuracy3: 1.0000, loss: 0.0324 ||: 100%|##########| 28/28 [00:06<00:00,  4.46it/s]\n","accuracy: 0.9972, accuracy3: 1.0000, loss: 0.0223 ||: 100%|##########| 28/28 [00:06<00:00,  4.50it/s]\n","accuracy: 0.9972, accuracy3: 1.0000, loss: 0.0157 ||: 100%|##########| 28/28 [00:06<00:00,  4.48it/s]\n","accuracy: 0.9994, accuracy3: 1.0000, loss: 0.0095 ||: 100%|##########| 28/28 [00:06<00:00,  4.32it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0070 ||: 100%|##########| 28/28 [00:06<00:00,  4.31it/s]\n","accuracy: 0.9994, accuracy3: 1.0000, loss: 0.0063 ||: 100%|##########| 28/28 [00:06<00:00,  4.31it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0058 ||:  32%|###2      | 9/28 [00:01<00:02,  6.74it/s]"],"name":"stderr"},{"output_type":"stream","text":["accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0050 ||: 100%|##########| 28/28 [00:06<00:00,  4.37it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0040 ||: 100%|##########| 28/28 [00:06<00:00,  4.28it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0034 ||: 100%|##########| 28/28 [00:06<00:00,  4.33it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0027 ||: 100%|##########| 28/28 [00:06<00:00,  4.34it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0023 ||: 100%|##########| 28/28 [00:06<00:00,  4.28it/s]\n","accuracy: 1.0000, accuracy3: 1.0000, loss: 0.0020 ||: 100%|##########| 28/28 [00:06<00:00,  4.35it/s]\n","accuracy: 0.82, accuracy3: 1.00 ||: 100%|##########| 19/19 [00:04<00:00,  3.95it/s]\n"],"name":"stderr"},{"output_type":"execute_result","data":{"text/plain":["Fetch20NewsgroupsClassifier(\n","  (model_text_field_embedder): BasicTextFieldEmbedder(\n","    (token_embedder_tokens): Embedding()\n","  )\n","  (internal_text_encoder): PytorchSeq2VecWrapper(\n","    (_module): LSTM(100, 100, batch_first=True, dropout=0.2, bidirectional=True)\n","  )\n","  (classifier_feedforward): FeedForward(\n","    (_linear_layers): ModuleList(\n","      (0): Linear(in_features=200, out_features=200, bias=True)\n","      (1): Linear(in_features=200, out_features=100, bias=True)\n","    )\n","    (_dropout): ModuleList(\n","      (0): Dropout(p=0.2)\n","      (1): Dropout(p=0.0)\n","    )\n","  )\n","  (loss): CrossEntropyLoss()\n",")"]},"metadata":{"tags":[]},"execution_count":9}]},{"metadata":{"id":"DiWqGLPwmxof","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}