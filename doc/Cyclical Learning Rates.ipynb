{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"Cyclical Learning Rates.ipynb","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"metadata":{"id":"pN8GJMWDnZfK","colab_type":"text"},"cell_type":"markdown","source":["https://github.com/pytorch/pytorch/pull/2016"]},{"metadata":{"id":"X75y1jQJnWOe","colab_type":"code","colab":{}},"cell_type":"code","source":["!pip3 install Pillow==4.0.0\n","!pip3 install torch==0.4.0\n","!pip3 install torchvision==0.2.1"],"execution_count":0,"outputs":[]},{"metadata":{"id":"8uwua0yJnk_2","colab_type":"code","colab":{}},"cell_type":"code","source":["import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","from torch.autograd import Variable\n","from torch.optim.lr_scheduler import LambdaLR, StepLR, MultiStepLR, ExponentialLR, CosineAnnealingLR, ReduceLROnPlateau\n","\n","import math\n","import numpy as np\n","import matplotlib.pyplot as plt\n","%matplotlib inline"],"execution_count":0,"outputs":[]},{"metadata":{"id":"5TU_nHo9nnDa","colab_type":"code","colab":{}},"cell_type":"code","source":["from __future__ import print_function\n","import argparse\n","import torch\n","import torch.nn as nn\n","import torch.nn.functional as F\n","import torch.optim as optim\n","from torchvision import datasets, transforms\n","\n","epochs = 20\n","batch_size = 100\n","log_interval = 10\n","device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n","\n","#モデルの定義\n","class Net(nn.Module):\n","    def __init__(self):\n","        super(Net, self).__init__()\n","        self.conv1 = nn.Conv2d(1, 10, kernel_size=5)\n","        self.conv2 = nn.Conv2d(10, 20, kernel_size=5)\n","        self.conv2_drop = nn.Dropout2d()\n","        self.fc1 = nn.Linear(320, 50)\n","        self.fc2 = nn.Linear(50, 10)\n","\n","    def forward(self, x):\n","        x = F.relu(F.max_pool2d(self.conv1(x), 2))\n","        x = F.relu(F.max_pool2d(self.conv2_drop(self.conv2(x)), 2))\n","        x = x.view(-1, 320)\n","        x = F.relu(self.fc1(x))\n","        x = F.dropout(x, training=self.training)\n","        x = self.fc2(x)\n","        return F.log_softmax(x, dim=1)\n","\n","#実行クラス\n","class Runner():\n","    def __init__(self, model):\n","        self.model = model\n","        self.scheduler = None\n","\n","    def train(self, model, device, train_loader, optimizer, epoch, use_scheduler=False):\n","        model.train()\n","        train_loss_list = []\n","        if use_scheduler == True:\n","            self.scheduler.step()\n","        for batch_idx, (data, target) in enumerate(train_loader):\n","            data, target = data.to(device), target.to(device)\n","            optimizer.zero_grad() #勾配クリア\n","            output = model(data)\n","            loss = F.nll_loss(output, target)\n","            loss.backward()  #誤差逆伝播の計算\n","            optimizer.step() #重みの更新\n","            \n","            if batch_idx % log_interval == 0:\n","                print('Train Epoch: {} [{}/{} ({:.0f}%)]\\tLoss: {:.6f}\\t Lr：{} '.format(\n","                    epoch, batch_idx * len(data), \n","                    len(train_loader.dataset),\n","                    100. * batch_idx / len(train_loader), \n","                    loss.item(), #lossはloss.item()\n","                    optimizer.param_groups[0][\"lr\"]\n","                ))\n","                train_loss_list.append(loss.item())\n","        return train_loss_list\n","\n","    def test(self, model, device, test_loader):\n","        model.eval()\n","        test_loss = 0\n","        correct = 0\n","        test_loss_list = []\n","        correct_list = []\n","        with torch.no_grad():\n","            for data, target in test_loader:\n","                data, target = data.to(device), target.to(device)\n","                output = model(data)\n","                test_loss += F.nll_loss(output, target, size_average=False).item() # sum up batch loss\n","                pred = output.max(1, keepdim=True)[1] # get the index of the max log-probability\n","                correct += pred.eq(target.view_as(pred)).sum().item()\n","\n","        test_loss /= len(test_loader.dataset)\n","        print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.0f}%)\\n'.format(\n","            test_loss, \n","            correct, \n","            len(test_loader.dataset),\n","            100. * correct / len(test_loader.dataset)\n","        ))\n","        correct_list.append(100. * correct / len(test_loader.dataset))\n","        test_loss_list.append(test_loss)\n","        return test_loss_list, correct_list"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QWpnMsSDnpiN","colab_type":"code","colab":{}},"cell_type":"code","source":["\n","train_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('data', train=True, download=True, transform=transforms.ToTensor()),\n","    batch_size=batch_size, shuffle=True)\n","\n","test_loader = torch.utils.data.DataLoader(\n","    datasets.MNIST('data', train=False, transform=transforms.ToTensor()),\n","    batch_size=200)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QodEEqyTnZrh","colab_type":"code","colab":{}},"cell_type":"code","source":["def cyclical_lr(step_sz, min_lr=0.001, max_lr=1, mode='triangular', scale_func=None, scale_md='cycles', gamma=1.):\n","    if scale_func == None:\n","        if mode == 'triangular':\n","            scale_fn = lambda x: 1.\n","            scale_mode = 'cycles'\n","        elif mode == 'triangular2':\n","            scale_fn = lambda x: 1 / (2.**(x - 1))\n","            scale_mode = 'cycles'\n","        elif mode == 'exp_range':\n","            scale_fn = lambda x: gamma**(x)\n","            scale_mode = 'iterations'\n","        else:\n","            raise ValueError(f'The {mode} is not valid value!')\n","    else:\n","        scale_fn = scale_func\n","        scale_mode = scale_md\n","\n","    lr_lambda = lambda iters: min_lr + (max_lr - min_lr) * rel_val(iters, step_sz, scale_mode)\n","\n","    def rel_val(iteration, stepsize, mode):\n","        cycle = math.floor(1 + iteration / (2 * stepsize))\n","        x = abs(iteration / stepsize - 2 * cycle + 1)\n","        if mode == 'cycles':\n","            return max(0, (1 - x)) * scale_fn(cycle)\n","        elif mode == 'iterations':\n","            return max(0, (1 - x)) * scale_fn(iteration)\n","        else:\n","            raise ValueError(f'The {scale_mode} is not valid value!')\n","\n","    return lr_lambda"],"execution_count":0,"outputs":[]},{"metadata":{"id":"QLw3d479nvie","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":9640},"outputId":"ee634e5f-cae1-49cd-cde4-c8490e58ded7","executionInfo":{"status":"error","timestamp":1535881023760,"user_tz":-540,"elapsed":86174,"user":{"displayName":"宮本圭一郎","photoUrl":"//lh5.googleusercontent.com/-5BLtx8oPSy8/AAAAAAAAAAI/AAAAAAAALtI/-tIwIsmAvCs/s50-c-k-no/photo.jpg","userId":"100227668169464343249"}}},"cell_type":"code","source":["model = Net()\n","model.cuda()\n","\n","train_loss_list = []\n","test_loss_list = []\n","correct_list = []\n","\n","runner = Runner(model)\n","\n","# optimizer = optim.SGD(model.parameters(), lr=0.01, momentum=0.5)\n","optimizer = optim.SGD(model.parameters(), lr=1.)\n","step_size = 2*len(train_loader)\n","# clr = cyclical_lr(step_size, min_lr=0.001, max_lr=1, mode='triangular2')\n","clr = cyclical_lr(step_size, min_lr=0.001, max_lr=0.005)\n","scheduler = LambdaLR(optimizer, [clr])\n","runner.scheduler = scheduler\n","\n","\n","def main(optimizer, model):\n","    for epoch in range(1, epochs + 1):\n","        ref_loss = runner.train(model, device, train_loader, optimizer, epoch)\n","        train_loss_list.extend(ref_loss)\n","        ref_loss, ref_correct = runner.test(model, device, test_loader)\n","        test_loss_list.extend(ref_loss)\n","        correct_list.extend(ref_correct)\n","    return train_loss_list, test_loss_list, correct_list\n","        \n","train_loss_list, test_loss_list, correct_list = main(optimizer, model)"],"execution_count":14,"outputs":[{"output_type":"stream","text":["Train Epoch: 1 [0/60000 (0%)]\tLoss: 2.326461\t Lr：0.001 \n","Train Epoch: 1 [1000/60000 (2%)]\tLoss: 2.321351\t Lr：0.001 \n","Train Epoch: 1 [2000/60000 (3%)]\tLoss: 2.311539\t Lr：0.001 \n","Train Epoch: 1 [3000/60000 (5%)]\tLoss: 2.320342\t Lr：0.001 \n","Train Epoch: 1 [4000/60000 (7%)]\tLoss: 2.327782\t Lr：0.001 \n","Train Epoch: 1 [5000/60000 (8%)]\tLoss: 2.323558\t Lr：0.001 \n","Train Epoch: 1 [6000/60000 (10%)]\tLoss: 2.321137\t Lr：0.001 \n","Train Epoch: 1 [7000/60000 (12%)]\tLoss: 2.311230\t Lr：0.001 \n","Train Epoch: 1 [8000/60000 (13%)]\tLoss: 2.317993\t Lr：0.001 \n","Train Epoch: 1 [9000/60000 (15%)]\tLoss: 2.328308\t Lr：0.001 \n","Train Epoch: 1 [10000/60000 (17%)]\tLoss: 2.302110\t Lr：0.001 \n","Train Epoch: 1 [11000/60000 (18%)]\tLoss: 2.312165\t Lr：0.001 \n","Train Epoch: 1 [12000/60000 (20%)]\tLoss: 2.328779\t Lr：0.001 \n","Train Epoch: 1 [13000/60000 (22%)]\tLoss: 2.318507\t Lr：0.001 \n","Train Epoch: 1 [14000/60000 (23%)]\tLoss: 2.334375\t Lr：0.001 \n","Train Epoch: 1 [15000/60000 (25%)]\tLoss: 2.310396\t Lr：0.001 \n","Train Epoch: 1 [16000/60000 (27%)]\tLoss: 2.300716\t Lr：0.001 \n","Train Epoch: 1 [17000/60000 (28%)]\tLoss: 2.293363\t Lr：0.001 \n","Train Epoch: 1 [18000/60000 (30%)]\tLoss: 2.309310\t Lr：0.001 \n","Train Epoch: 1 [19000/60000 (32%)]\tLoss: 2.331915\t Lr：0.001 \n","Train Epoch: 1 [20000/60000 (33%)]\tLoss: 2.322518\t Lr：0.001 \n","Train Epoch: 1 [21000/60000 (35%)]\tLoss: 2.308277\t Lr：0.001 \n","Train Epoch: 1 [22000/60000 (37%)]\tLoss: 2.317237\t Lr：0.001 \n","Train Epoch: 1 [23000/60000 (38%)]\tLoss: 2.297280\t Lr：0.001 \n","Train Epoch: 1 [24000/60000 (40%)]\tLoss: 2.314109\t Lr：0.001 \n","Train Epoch: 1 [25000/60000 (42%)]\tLoss: 2.317623\t Lr：0.001 \n","Train Epoch: 1 [26000/60000 (43%)]\tLoss: 2.331209\t Lr：0.001 \n","Train Epoch: 1 [27000/60000 (45%)]\tLoss: 2.312886\t Lr：0.001 \n","Train Epoch: 1 [28000/60000 (47%)]\tLoss: 2.293000\t Lr：0.001 \n","Train Epoch: 1 [29000/60000 (48%)]\tLoss: 2.334095\t Lr：0.001 \n","Train Epoch: 1 [30000/60000 (50%)]\tLoss: 2.300917\t Lr：0.001 \n","Train Epoch: 1 [31000/60000 (52%)]\tLoss: 2.309740\t Lr：0.001 \n","Train Epoch: 1 [32000/60000 (53%)]\tLoss: 2.316531\t Lr：0.001 \n","Train Epoch: 1 [33000/60000 (55%)]\tLoss: 2.317381\t Lr：0.001 \n","Train Epoch: 1 [34000/60000 (57%)]\tLoss: 2.319634\t Lr：0.001 \n","Train Epoch: 1 [35000/60000 (58%)]\tLoss: 2.311238\t Lr：0.001 \n","Train Epoch: 1 [36000/60000 (60%)]\tLoss: 2.310634\t Lr：0.001 \n","Train Epoch: 1 [37000/60000 (62%)]\tLoss: 2.313602\t Lr：0.001 \n","Train Epoch: 1 [38000/60000 (63%)]\tLoss: 2.317823\t Lr：0.001 \n","Train Epoch: 1 [39000/60000 (65%)]\tLoss: 2.306305\t Lr：0.001 \n","Train Epoch: 1 [40000/60000 (67%)]\tLoss: 2.319158\t Lr：0.001 \n","Train Epoch: 1 [41000/60000 (68%)]\tLoss: 2.321876\t Lr：0.001 \n","Train Epoch: 1 [42000/60000 (70%)]\tLoss: 2.301620\t Lr：0.001 \n","Train Epoch: 1 [43000/60000 (72%)]\tLoss: 2.315343\t Lr：0.001 \n","Train Epoch: 1 [44000/60000 (73%)]\tLoss: 2.292106\t Lr：0.001 \n","Train Epoch: 1 [45000/60000 (75%)]\tLoss: 2.306872\t Lr：0.001 \n","Train Epoch: 1 [46000/60000 (77%)]\tLoss: 2.301195\t Lr：0.001 \n","Train Epoch: 1 [47000/60000 (78%)]\tLoss: 2.313362\t Lr：0.001 \n","Train Epoch: 1 [48000/60000 (80%)]\tLoss: 2.304397\t Lr：0.001 \n","Train Epoch: 1 [49000/60000 (82%)]\tLoss: 2.301771\t Lr：0.001 \n","Train Epoch: 1 [50000/60000 (83%)]\tLoss: 2.313081\t Lr：0.001 \n","Train Epoch: 1 [51000/60000 (85%)]\tLoss: 2.321521\t Lr：0.001 \n","Train Epoch: 1 [52000/60000 (87%)]\tLoss: 2.302382\t Lr：0.001 \n","Train Epoch: 1 [53000/60000 (88%)]\tLoss: 2.308104\t Lr：0.001 \n","Train Epoch: 1 [54000/60000 (90%)]\tLoss: 2.322646\t Lr：0.001 \n","Train Epoch: 1 [55000/60000 (92%)]\tLoss: 2.291932\t Lr：0.001 \n","Train Epoch: 1 [56000/60000 (93%)]\tLoss: 2.305734\t Lr：0.001 \n","Train Epoch: 1 [57000/60000 (95%)]\tLoss: 2.290628\t Lr：0.001 \n","Train Epoch: 1 [58000/60000 (97%)]\tLoss: 2.331509\t Lr：0.001 \n","Train Epoch: 1 [59000/60000 (98%)]\tLoss: 2.296664\t Lr：0.001 \n","\n","Test set: Average loss: 2.3055, Accuracy: 883/10000 (9%)\n","\n","Train Epoch: 2 [0/60000 (0%)]\tLoss: 2.303028\t Lr：0.001 \n","Train Epoch: 2 [1000/60000 (2%)]\tLoss: 2.304119\t Lr：0.001 \n","Train Epoch: 2 [2000/60000 (3%)]\tLoss: 2.297476\t Lr：0.001 \n","Train Epoch: 2 [3000/60000 (5%)]\tLoss: 2.316572\t Lr：0.001 \n","Train Epoch: 2 [4000/60000 (7%)]\tLoss: 2.332115\t Lr：0.001 \n","Train Epoch: 2 [5000/60000 (8%)]\tLoss: 2.297166\t Lr：0.001 \n","Train Epoch: 2 [6000/60000 (10%)]\tLoss: 2.304386\t Lr：0.001 \n","Train Epoch: 2 [7000/60000 (12%)]\tLoss: 2.294783\t Lr：0.001 \n","Train Epoch: 2 [8000/60000 (13%)]\tLoss: 2.304355\t Lr：0.001 \n","Train Epoch: 2 [9000/60000 (15%)]\tLoss: 2.306356\t Lr：0.001 \n","Train Epoch: 2 [10000/60000 (17%)]\tLoss: 2.284461\t Lr：0.001 \n","Train Epoch: 2 [11000/60000 (18%)]\tLoss: 2.310355\t Lr：0.001 \n","Train Epoch: 2 [12000/60000 (20%)]\tLoss: 2.296095\t Lr：0.001 \n","Train Epoch: 2 [13000/60000 (22%)]\tLoss: 2.296222\t Lr：0.001 \n","Train Epoch: 2 [14000/60000 (23%)]\tLoss: 2.291734\t Lr：0.001 \n","Train Epoch: 2 [15000/60000 (25%)]\tLoss: 2.322658\t Lr：0.001 \n","Train Epoch: 2 [16000/60000 (27%)]\tLoss: 2.305772\t Lr：0.001 \n","Train Epoch: 2 [17000/60000 (28%)]\tLoss: 2.309610\t Lr：0.001 \n","Train Epoch: 2 [18000/60000 (30%)]\tLoss: 2.305197\t Lr：0.001 \n","Train Epoch: 2 [19000/60000 (32%)]\tLoss: 2.301572\t Lr：0.001 \n","Train Epoch: 2 [20000/60000 (33%)]\tLoss: 2.300852\t Lr：0.001 \n","Train Epoch: 2 [21000/60000 (35%)]\tLoss: 2.312602\t Lr：0.001 \n","Train Epoch: 2 [22000/60000 (37%)]\tLoss: 2.304425\t Lr：0.001 \n","Train Epoch: 2 [23000/60000 (38%)]\tLoss: 2.294599\t Lr：0.001 \n","Train Epoch: 2 [24000/60000 (40%)]\tLoss: 2.322200\t Lr：0.001 \n","Train Epoch: 2 [25000/60000 (42%)]\tLoss: 2.304795\t Lr：0.001 \n","Train Epoch: 2 [26000/60000 (43%)]\tLoss: 2.316151\t Lr：0.001 \n","Train Epoch: 2 [27000/60000 (45%)]\tLoss: 2.310498\t Lr：0.001 \n","Train Epoch: 2 [28000/60000 (47%)]\tLoss: 2.298433\t Lr：0.001 \n","Train Epoch: 2 [29000/60000 (48%)]\tLoss: 2.291870\t Lr：0.001 \n","Train Epoch: 2 [30000/60000 (50%)]\tLoss: 2.288949\t Lr：0.001 \n","Train Epoch: 2 [31000/60000 (52%)]\tLoss: 2.290208\t Lr：0.001 \n","Train Epoch: 2 [32000/60000 (53%)]\tLoss: 2.313481\t Lr：0.001 \n","Train Epoch: 2 [33000/60000 (55%)]\tLoss: 2.310333\t Lr：0.001 \n","Train Epoch: 2 [34000/60000 (57%)]\tLoss: 2.312873\t Lr：0.001 \n","Train Epoch: 2 [35000/60000 (58%)]\tLoss: 2.281229\t Lr：0.001 \n","Train Epoch: 2 [36000/60000 (60%)]\tLoss: 2.308693\t Lr：0.001 \n","Train Epoch: 2 [37000/60000 (62%)]\tLoss: 2.307147\t Lr：0.001 \n","Train Epoch: 2 [38000/60000 (63%)]\tLoss: 2.290388\t Lr：0.001 \n","Train Epoch: 2 [39000/60000 (65%)]\tLoss: 2.301994\t Lr：0.001 \n","Train Epoch: 2 [40000/60000 (67%)]\tLoss: 2.306404\t Lr：0.001 \n","Train Epoch: 2 [41000/60000 (68%)]\tLoss: 2.299200\t Lr：0.001 \n","Train Epoch: 2 [42000/60000 (70%)]\tLoss: 2.293761\t Lr：0.001 \n","Train Epoch: 2 [43000/60000 (72%)]\tLoss: 2.302099\t Lr：0.001 \n","Train Epoch: 2 [44000/60000 (73%)]\tLoss: 2.299390\t Lr：0.001 \n","Train Epoch: 2 [45000/60000 (75%)]\tLoss: 2.297792\t Lr：0.001 \n","Train Epoch: 2 [46000/60000 (77%)]\tLoss: 2.308918\t Lr：0.001 \n","Train Epoch: 2 [47000/60000 (78%)]\tLoss: 2.313800\t Lr：0.001 \n","Train Epoch: 2 [48000/60000 (80%)]\tLoss: 2.302257\t Lr：0.001 \n","Train Epoch: 2 [49000/60000 (82%)]\tLoss: 2.296606\t Lr：0.001 \n","Train Epoch: 2 [50000/60000 (83%)]\tLoss: 2.295792\t Lr：0.001 \n","Train Epoch: 2 [51000/60000 (85%)]\tLoss: 2.295919\t Lr：0.001 \n","Train Epoch: 2 [52000/60000 (87%)]\tLoss: 2.310115\t Lr：0.001 \n","Train Epoch: 2 [53000/60000 (88%)]\tLoss: 2.301198\t Lr：0.001 \n","Train Epoch: 2 [54000/60000 (90%)]\tLoss: 2.287840\t Lr：0.001 \n","Train Epoch: 2 [55000/60000 (92%)]\tLoss: 2.294606\t Lr：0.001 \n","Train Epoch: 2 [56000/60000 (93%)]\tLoss: 2.300296\t Lr：0.001 \n","Train Epoch: 2 [57000/60000 (95%)]\tLoss: 2.303091\t Lr：0.001 \n","Train Epoch: 2 [58000/60000 (97%)]\tLoss: 2.291020\t Lr：0.001 \n","Train Epoch: 2 [59000/60000 (98%)]\tLoss: 2.284987\t Lr：0.001 \n","\n","Test set: Average loss: 2.2981, Accuracy: 940/10000 (9%)\n","\n","Train Epoch: 3 [0/60000 (0%)]\tLoss: 2.309584\t Lr：0.001 \n","Train Epoch: 3 [1000/60000 (2%)]\tLoss: 2.293281\t Lr：0.001 \n","Train Epoch: 3 [2000/60000 (3%)]\tLoss: 2.299348\t Lr：0.001 \n","Train Epoch: 3 [3000/60000 (5%)]\tLoss: 2.305148\t Lr：0.001 \n","Train Epoch: 3 [4000/60000 (7%)]\tLoss: 2.305964\t Lr：0.001 \n","Train Epoch: 3 [5000/60000 (8%)]\tLoss: 2.294455\t Lr：0.001 \n","Train Epoch: 3 [6000/60000 (10%)]\tLoss: 2.296309\t Lr：0.001 \n","Train Epoch: 3 [7000/60000 (12%)]\tLoss: 2.294144\t Lr：0.001 \n","Train Epoch: 3 [8000/60000 (13%)]\tLoss: 2.306626\t Lr：0.001 \n","Train Epoch: 3 [9000/60000 (15%)]\tLoss: 2.294030\t Lr：0.001 \n","Train Epoch: 3 [10000/60000 (17%)]\tLoss: 2.297639\t Lr：0.001 \n","Train Epoch: 3 [11000/60000 (18%)]\tLoss: 2.297152\t Lr：0.001 \n","Train Epoch: 3 [12000/60000 (20%)]\tLoss: 2.295749\t Lr：0.001 \n","Train Epoch: 3 [13000/60000 (22%)]\tLoss: 2.309397\t Lr：0.001 \n","Train Epoch: 3 [14000/60000 (23%)]\tLoss: 2.309850\t Lr：0.001 \n","Train Epoch: 3 [15000/60000 (25%)]\tLoss: 2.300475\t Lr：0.001 \n","Train Epoch: 3 [16000/60000 (27%)]\tLoss: 2.288620\t Lr：0.001 \n","Train Epoch: 3 [17000/60000 (28%)]\tLoss: 2.299911\t Lr：0.001 \n","Train Epoch: 3 [18000/60000 (30%)]\tLoss: 2.281260\t Lr：0.001 \n","Train Epoch: 3 [19000/60000 (32%)]\tLoss: 2.294448\t Lr：0.001 \n","Train Epoch: 3 [20000/60000 (33%)]\tLoss: 2.280389\t Lr：0.001 \n","Train Epoch: 3 [21000/60000 (35%)]\tLoss: 2.293393\t Lr：0.001 \n","Train Epoch: 3 [22000/60000 (37%)]\tLoss: 2.309527\t Lr：0.001 \n","Train Epoch: 3 [23000/60000 (38%)]\tLoss: 2.290672\t Lr：0.001 \n","Train Epoch: 3 [24000/60000 (40%)]\tLoss: 2.294039\t Lr：0.001 \n","Train Epoch: 3 [25000/60000 (42%)]\tLoss: 2.305434\t Lr：0.001 \n","Train Epoch: 3 [26000/60000 (43%)]\tLoss: 2.302522\t Lr：0.001 \n","Train Epoch: 3 [27000/60000 (45%)]\tLoss: 2.288372\t Lr：0.001 \n","Train Epoch: 3 [28000/60000 (47%)]\tLoss: 2.299135\t Lr：0.001 \n","Train Epoch: 3 [29000/60000 (48%)]\tLoss: 2.268584\t Lr：0.001 \n","Train Epoch: 3 [30000/60000 (50%)]\tLoss: 2.296604\t Lr：0.001 \n","Train Epoch: 3 [31000/60000 (52%)]\tLoss: 2.300870\t Lr：0.001 \n","Train Epoch: 3 [32000/60000 (53%)]\tLoss: 2.291971\t Lr：0.001 \n","Train Epoch: 3 [33000/60000 (55%)]\tLoss: 2.292839\t Lr：0.001 \n","Train Epoch: 3 [34000/60000 (57%)]\tLoss: 2.294463\t Lr：0.001 \n","Train Epoch: 3 [35000/60000 (58%)]\tLoss: 2.306245\t Lr：0.001 \n","Train Epoch: 3 [36000/60000 (60%)]\tLoss: 2.290707\t Lr：0.001 \n","Train Epoch: 3 [37000/60000 (62%)]\tLoss: 2.294163\t Lr：0.001 \n","Train Epoch: 3 [38000/60000 (63%)]\tLoss: 2.297727\t Lr：0.001 \n","Train Epoch: 3 [39000/60000 (65%)]\tLoss: 2.287231\t Lr：0.001 \n","Train Epoch: 3 [40000/60000 (67%)]\tLoss: 2.294884\t Lr：0.001 \n","Train Epoch: 3 [41000/60000 (68%)]\tLoss: 2.304132\t Lr：0.001 \n","Train Epoch: 3 [42000/60000 (70%)]\tLoss: 2.297989\t Lr：0.001 \n","Train Epoch: 3 [43000/60000 (72%)]\tLoss: 2.284607\t Lr：0.001 \n","Train Epoch: 3 [44000/60000 (73%)]\tLoss: 2.304192\t Lr：0.001 \n","Train Epoch: 3 [45000/60000 (75%)]\tLoss: 2.301327\t Lr：0.001 \n","Train Epoch: 3 [46000/60000 (77%)]\tLoss: 2.295082\t Lr：0.001 \n","Train Epoch: 3 [47000/60000 (78%)]\tLoss: 2.281889\t Lr：0.001 \n","Train Epoch: 3 [48000/60000 (80%)]\tLoss: 2.286147\t Lr：0.001 \n","Train Epoch: 3 [49000/60000 (82%)]\tLoss: 2.289368\t Lr：0.001 \n","Train Epoch: 3 [50000/60000 (83%)]\tLoss: 2.282642\t Lr：0.001 \n","Train Epoch: 3 [51000/60000 (85%)]\tLoss: 2.304762\t Lr：0.001 \n","Train Epoch: 3 [52000/60000 (87%)]\tLoss: 2.303293\t Lr：0.001 \n","Train Epoch: 3 [53000/60000 (88%)]\tLoss: 2.281316\t Lr：0.001 \n","Train Epoch: 3 [54000/60000 (90%)]\tLoss: 2.286794\t Lr：0.001 \n","Train Epoch: 3 [55000/60000 (92%)]\tLoss: 2.282540\t Lr：0.001 \n","Train Epoch: 3 [56000/60000 (93%)]\tLoss: 2.299380\t Lr：0.001 \n","Train Epoch: 3 [57000/60000 (95%)]\tLoss: 2.289883\t Lr：0.001 \n","Train Epoch: 3 [58000/60000 (97%)]\tLoss: 2.308053\t Lr：0.001 \n","Train Epoch: 3 [59000/60000 (98%)]\tLoss: 2.297727\t Lr：0.001 \n","\n","Test set: Average loss: 2.2906, Accuracy: 1125/10000 (11%)\n","\n","Train Epoch: 4 [0/60000 (0%)]\tLoss: 2.296333\t Lr：0.001 \n","Train Epoch: 4 [1000/60000 (2%)]\tLoss: 2.283598\t Lr：0.001 \n","Train Epoch: 4 [2000/60000 (3%)]\tLoss: 2.302216\t Lr：0.001 \n","Train Epoch: 4 [3000/60000 (5%)]\tLoss: 2.285406\t Lr：0.001 \n","Train Epoch: 4 [4000/60000 (7%)]\tLoss: 2.304703\t Lr：0.001 \n","Train Epoch: 4 [5000/60000 (8%)]\tLoss: 2.290951\t Lr：0.001 \n","Train Epoch: 4 [6000/60000 (10%)]\tLoss: 2.295407\t Lr：0.001 \n","Train Epoch: 4 [7000/60000 (12%)]\tLoss: 2.300126\t Lr：0.001 \n","Train Epoch: 4 [8000/60000 (13%)]\tLoss: 2.291833\t Lr：0.001 \n","Train Epoch: 4 [9000/60000 (15%)]\tLoss: 2.284790\t Lr：0.001 \n","Train Epoch: 4 [10000/60000 (17%)]\tLoss: 2.294549\t Lr：0.001 \n","Train Epoch: 4 [11000/60000 (18%)]\tLoss: 2.308435\t Lr：0.001 \n","Train Epoch: 4 [12000/60000 (20%)]\tLoss: 2.290750\t Lr：0.001 \n","Train Epoch: 4 [13000/60000 (22%)]\tLoss: 2.298737\t Lr：0.001 \n","Train Epoch: 4 [14000/60000 (23%)]\tLoss: 2.304708\t Lr：0.001 \n","Train Epoch: 4 [15000/60000 (25%)]\tLoss: 2.304551\t Lr：0.001 \n","Train Epoch: 4 [16000/60000 (27%)]\tLoss: 2.270952\t Lr：0.001 \n","Train Epoch: 4 [17000/60000 (28%)]\tLoss: 2.289181\t Lr：0.001 \n","Train Epoch: 4 [18000/60000 (30%)]\tLoss: 2.282774\t Lr：0.001 \n","Train Epoch: 4 [19000/60000 (32%)]\tLoss: 2.294419\t Lr：0.001 \n","Train Epoch: 4 [20000/60000 (33%)]\tLoss: 2.278914\t Lr：0.001 \n","Train Epoch: 4 [21000/60000 (35%)]\tLoss: 2.305339\t Lr：0.001 \n","Train Epoch: 4 [22000/60000 (37%)]\tLoss: 2.286472\t Lr：0.001 \n","Train Epoch: 4 [23000/60000 (38%)]\tLoss: 2.292145\t Lr：0.001 \n","Train Epoch: 4 [24000/60000 (40%)]\tLoss: 2.299719\t Lr：0.001 \n","Train Epoch: 4 [25000/60000 (42%)]\tLoss: 2.297691\t Lr：0.001 \n","Train Epoch: 4 [26000/60000 (43%)]\tLoss: 2.290876\t Lr：0.001 \n","Train Epoch: 4 [27000/60000 (45%)]\tLoss: 2.291996\t Lr：0.001 \n","Train Epoch: 4 [28000/60000 (47%)]\tLoss: 2.288222\t Lr：0.001 \n","Train Epoch: 4 [29000/60000 (48%)]\tLoss: 2.290319\t Lr：0.001 \n","Train Epoch: 4 [30000/60000 (50%)]\tLoss: 2.304229\t Lr：0.001 \n","Train Epoch: 4 [31000/60000 (52%)]\tLoss: 2.294742\t Lr：0.001 \n","Train Epoch: 4 [32000/60000 (53%)]\tLoss: 2.295320\t Lr：0.001 \n","Train Epoch: 4 [33000/60000 (55%)]\tLoss: 2.276419\t Lr：0.001 \n","Train Epoch: 4 [34000/60000 (57%)]\tLoss: 2.292154\t Lr：0.001 \n","Train Epoch: 4 [35000/60000 (58%)]\tLoss: 2.284013\t Lr：0.001 \n","Train Epoch: 4 [36000/60000 (60%)]\tLoss: 2.286753\t Lr：0.001 \n","Train Epoch: 4 [37000/60000 (62%)]\tLoss: 2.287247\t Lr：0.001 \n","Train Epoch: 4 [38000/60000 (63%)]\tLoss: 2.286619\t Lr：0.001 \n","Train Epoch: 4 [39000/60000 (65%)]\tLoss: 2.293566\t Lr：0.001 \n","Train Epoch: 4 [40000/60000 (67%)]\tLoss: 2.296073\t Lr：0.001 \n","Train Epoch: 4 [41000/60000 (68%)]\tLoss: 2.286613\t Lr：0.001 \n","Train Epoch: 4 [42000/60000 (70%)]\tLoss: 2.294842\t Lr：0.001 \n","Train Epoch: 4 [43000/60000 (72%)]\tLoss: 2.287924\t Lr：0.001 \n","Train Epoch: 4 [44000/60000 (73%)]\tLoss: 2.296019\t Lr：0.001 \n","Train Epoch: 4 [45000/60000 (75%)]\tLoss: 2.285449\t Lr：0.001 \n","Train Epoch: 4 [46000/60000 (77%)]\tLoss: 2.273813\t Lr：0.001 \n","Train Epoch: 4 [47000/60000 (78%)]\tLoss: 2.297016\t Lr：0.001 \n","Train Epoch: 4 [48000/60000 (80%)]\tLoss: 2.285227\t Lr：0.001 \n","Train Epoch: 4 [49000/60000 (82%)]\tLoss: 2.279229\t Lr：0.001 \n","Train Epoch: 4 [50000/60000 (83%)]\tLoss: 2.282301\t Lr：0.001 \n","Train Epoch: 4 [51000/60000 (85%)]\tLoss: 2.284529\t Lr：0.001 \n","Train Epoch: 4 [52000/60000 (87%)]\tLoss: 2.282839\t Lr：0.001 \n","Train Epoch: 4 [53000/60000 (88%)]\tLoss: 2.296933\t Lr：0.001 \n","Train Epoch: 4 [54000/60000 (90%)]\tLoss: 2.274986\t Lr：0.001 \n","Train Epoch: 4 [55000/60000 (92%)]\tLoss: 2.290634\t Lr：0.001 \n","Train Epoch: 4 [56000/60000 (93%)]\tLoss: 2.265042\t Lr：0.001 \n","Train Epoch: 4 [57000/60000 (95%)]\tLoss: 2.277570\t Lr：0.001 \n","Train Epoch: 4 [58000/60000 (97%)]\tLoss: 2.287860\t Lr：0.001 \n","Train Epoch: 4 [59000/60000 (98%)]\tLoss: 2.284612\t Lr：0.001 \n","\n","Test set: Average loss: 2.2811, Accuracy: 1833/10000 (18%)\n","\n","Train Epoch: 5 [0/60000 (0%)]\tLoss: 2.284979\t Lr：0.001 \n","Train Epoch: 5 [1000/60000 (2%)]\tLoss: 2.282963\t Lr：0.001 \n","Train Epoch: 5 [2000/60000 (3%)]\tLoss: 2.281808\t Lr：0.001 \n","Train Epoch: 5 [3000/60000 (5%)]\tLoss: 2.290023\t Lr：0.001 \n","Train Epoch: 5 [4000/60000 (7%)]\tLoss: 2.284747\t Lr：0.001 \n","Train Epoch: 5 [5000/60000 (8%)]\tLoss: 2.288475\t Lr：0.001 \n","Train Epoch: 5 [6000/60000 (10%)]\tLoss: 2.285532\t Lr：0.001 \n","Train Epoch: 5 [7000/60000 (12%)]\tLoss: 2.312296\t Lr：0.001 \n","Train Epoch: 5 [8000/60000 (13%)]\tLoss: 2.285034\t Lr：0.001 \n","Train Epoch: 5 [9000/60000 (15%)]\tLoss: 2.277298\t Lr：0.001 \n","Train Epoch: 5 [10000/60000 (17%)]\tLoss: 2.292628\t Lr：0.001 \n","Train Epoch: 5 [11000/60000 (18%)]\tLoss: 2.284678\t Lr：0.001 \n","Train Epoch: 5 [12000/60000 (20%)]\tLoss: 2.276015\t Lr：0.001 \n","Train Epoch: 5 [13000/60000 (22%)]\tLoss: 2.276047\t Lr：0.001 \n","Train Epoch: 5 [14000/60000 (23%)]\tLoss: 2.272170\t Lr：0.001 \n","Train Epoch: 5 [15000/60000 (25%)]\tLoss: 2.280549\t Lr：0.001 \n","Train Epoch: 5 [16000/60000 (27%)]\tLoss: 2.284326\t Lr：0.001 \n","Train Epoch: 5 [17000/60000 (28%)]\tLoss: 2.293104\t Lr：0.001 \n","Train Epoch: 5 [18000/60000 (30%)]\tLoss: 2.275743\t Lr：0.001 \n","Train Epoch: 5 [19000/60000 (32%)]\tLoss: 2.282980\t Lr：0.001 \n","Train Epoch: 5 [20000/60000 (33%)]\tLoss: 2.281550\t Lr：0.001 \n","Train Epoch: 5 [21000/60000 (35%)]\tLoss: 2.303367\t Lr：0.001 \n","Train Epoch: 5 [22000/60000 (37%)]\tLoss: 2.279671\t Lr：0.001 \n","Train Epoch: 5 [23000/60000 (38%)]\tLoss: 2.280425\t Lr：0.001 \n","Train Epoch: 5 [24000/60000 (40%)]\tLoss: 2.264103\t Lr：0.001 \n","Train Epoch: 5 [25000/60000 (42%)]\tLoss: 2.289211\t Lr：0.001 \n","Train Epoch: 5 [26000/60000 (43%)]\tLoss: 2.281851\t Lr：0.001 \n","Train Epoch: 5 [27000/60000 (45%)]\tLoss: 2.275208\t Lr：0.001 \n","Train Epoch: 5 [28000/60000 (47%)]\tLoss: 2.270535\t Lr：0.001 \n","Train Epoch: 5 [29000/60000 (48%)]\tLoss: 2.291440\t Lr：0.001 \n","Train Epoch: 5 [30000/60000 (50%)]\tLoss: 2.285400\t Lr：0.001 \n","Train Epoch: 5 [31000/60000 (52%)]\tLoss: 2.270540\t Lr：0.001 \n","Train Epoch: 5 [32000/60000 (53%)]\tLoss: 2.265021\t Lr：0.001 \n","Train Epoch: 5 [33000/60000 (55%)]\tLoss: 2.277273\t Lr：0.001 \n","Train Epoch: 5 [34000/60000 (57%)]\tLoss: 2.279227\t Lr：0.001 \n","Train Epoch: 5 [35000/60000 (58%)]\tLoss: 2.275044\t Lr：0.001 \n","Train Epoch: 5 [36000/60000 (60%)]\tLoss: 2.274715\t Lr：0.001 \n","Train Epoch: 5 [37000/60000 (62%)]\tLoss: 2.273176\t Lr：0.001 \n","Train Epoch: 5 [38000/60000 (63%)]\tLoss: 2.275970\t Lr：0.001 \n","Train Epoch: 5 [39000/60000 (65%)]\tLoss: 2.279224\t Lr：0.001 \n","Train Epoch: 5 [40000/60000 (67%)]\tLoss: 2.286501\t Lr：0.001 \n","Train Epoch: 5 [41000/60000 (68%)]\tLoss: 2.276926\t Lr：0.001 \n","Train Epoch: 5 [42000/60000 (70%)]\tLoss: 2.272784\t Lr：0.001 \n","Train Epoch: 5 [43000/60000 (72%)]\tLoss: 2.273713\t Lr：0.001 \n","Train Epoch: 5 [44000/60000 (73%)]\tLoss: 2.270747\t Lr：0.001 \n","Train Epoch: 5 [45000/60000 (75%)]\tLoss: 2.286023\t Lr：0.001 \n","Train Epoch: 5 [46000/60000 (77%)]\tLoss: 2.270987\t Lr：0.001 \n","Train Epoch: 5 [47000/60000 (78%)]\tLoss: 2.273582\t Lr：0.001 \n","Train Epoch: 5 [48000/60000 (80%)]\tLoss: 2.256092\t Lr：0.001 \n","Train Epoch: 5 [49000/60000 (82%)]\tLoss: 2.274185\t Lr：0.001 \n","Train Epoch: 5 [50000/60000 (83%)]\tLoss: 2.274708\t Lr：0.001 \n","Train Epoch: 5 [51000/60000 (85%)]\tLoss: 2.277110\t Lr：0.001 \n","Train Epoch: 5 [52000/60000 (87%)]\tLoss: 2.287394\t Lr：0.001 \n","Train Epoch: 5 [53000/60000 (88%)]\tLoss: 2.284467\t Lr：0.001 \n","Train Epoch: 5 [54000/60000 (90%)]\tLoss: 2.270412\t Lr：0.001 \n","Train Epoch: 5 [55000/60000 (92%)]\tLoss: 2.267719\t Lr：0.001 \n","Train Epoch: 5 [56000/60000 (93%)]\tLoss: 2.273322\t Lr：0.001 \n","Train Epoch: 5 [57000/60000 (95%)]\tLoss: 2.263262\t Lr：0.001 \n","Train Epoch: 5 [58000/60000 (97%)]\tLoss: 2.269400\t Lr：0.001 \n","Train Epoch: 5 [59000/60000 (98%)]\tLoss: 2.287688\t Lr：0.001 \n","\n","Test set: Average loss: 2.2670, Accuracy: 2537/10000 (25%)\n","\n","Train Epoch: 6 [0/60000 (0%)]\tLoss: 2.271487\t Lr：0.001 \n","Train Epoch: 6 [1000/60000 (2%)]\tLoss: 2.271827\t Lr：0.001 \n","Train Epoch: 6 [2000/60000 (3%)]\tLoss: 2.261879\t Lr：0.001 \n","Train Epoch: 6 [3000/60000 (5%)]\tLoss: 2.282743\t Lr：0.001 \n","Train Epoch: 6 [4000/60000 (7%)]\tLoss: 2.259333\t Lr：0.001 \n","Train Epoch: 6 [5000/60000 (8%)]\tLoss: 2.285407\t Lr：0.001 \n","Train Epoch: 6 [6000/60000 (10%)]\tLoss: 2.288386\t Lr：0.001 \n","Train Epoch: 6 [7000/60000 (12%)]\tLoss: 2.271053\t Lr：0.001 \n","Train Epoch: 6 [8000/60000 (13%)]\tLoss: 2.253716\t Lr：0.001 \n","Train Epoch: 6 [9000/60000 (15%)]\tLoss: 2.282123\t Lr：0.001 \n","Train Epoch: 6 [10000/60000 (17%)]\tLoss: 2.261530\t Lr：0.001 \n","Train Epoch: 6 [11000/60000 (18%)]\tLoss: 2.278655\t Lr：0.001 \n","Train Epoch: 6 [12000/60000 (20%)]\tLoss: 2.279500\t Lr：0.001 \n","Train Epoch: 6 [13000/60000 (22%)]\tLoss: 2.266473\t Lr：0.001 \n","Train Epoch: 6 [14000/60000 (23%)]\tLoss: 2.255575\t Lr：0.001 \n","Train Epoch: 6 [15000/60000 (25%)]\tLoss: 2.281082\t Lr：0.001 \n","Train Epoch: 6 [16000/60000 (27%)]\tLoss: 2.269961\t Lr：0.001 \n","Train Epoch: 6 [17000/60000 (28%)]\tLoss: 2.276446\t Lr：0.001 \n","Train Epoch: 6 [18000/60000 (30%)]\tLoss: 2.274766\t Lr：0.001 \n","Train Epoch: 6 [19000/60000 (32%)]\tLoss: 2.283083\t Lr：0.001 \n","Train Epoch: 6 [20000/60000 (33%)]\tLoss: 2.286676\t Lr：0.001 \n","Train Epoch: 6 [21000/60000 (35%)]\tLoss: 2.252616\t Lr：0.001 \n","Train Epoch: 6 [22000/60000 (37%)]\tLoss: 2.273653\t Lr：0.001 \n","Train Epoch: 6 [23000/60000 (38%)]\tLoss: 2.278881\t Lr：0.001 \n","Train Epoch: 6 [24000/60000 (40%)]\tLoss: 2.270853\t Lr：0.001 \n","Train Epoch: 6 [25000/60000 (42%)]\tLoss: 2.266495\t Lr：0.001 \n","Train Epoch: 6 [26000/60000 (43%)]\tLoss: 2.270797\t Lr：0.001 \n","Train Epoch: 6 [27000/60000 (45%)]\tLoss: 2.256609\t Lr：0.001 \n","Train Epoch: 6 [28000/60000 (47%)]\tLoss: 2.254102\t Lr：0.001 \n","Train Epoch: 6 [29000/60000 (48%)]\tLoss: 2.266339\t Lr：0.001 \n","Train Epoch: 6 [30000/60000 (50%)]\tLoss: 2.260716\t Lr：0.001 \n","Train Epoch: 6 [31000/60000 (52%)]\tLoss: 2.270661\t Lr：0.001 \n","Train Epoch: 6 [32000/60000 (53%)]\tLoss: 2.252261\t Lr：0.001 \n","Train Epoch: 6 [33000/60000 (55%)]\tLoss: 2.263425\t Lr：0.001 \n","Train Epoch: 6 [34000/60000 (57%)]\tLoss: 2.246615\t Lr：0.001 \n","Train Epoch: 6 [35000/60000 (58%)]\tLoss: 2.271097\t Lr：0.001 \n","Train Epoch: 6 [36000/60000 (60%)]\tLoss: 2.255305\t Lr：0.001 \n","Train Epoch: 6 [37000/60000 (62%)]\tLoss: 2.261432\t Lr：0.001 \n","Train Epoch: 6 [38000/60000 (63%)]\tLoss: 2.275550\t Lr：0.001 \n","Train Epoch: 6 [39000/60000 (65%)]\tLoss: 2.239728\t Lr：0.001 \n","Train Epoch: 6 [40000/60000 (67%)]\tLoss: 2.268291\t Lr：0.001 \n","Train Epoch: 6 [41000/60000 (68%)]\tLoss: 2.268238\t Lr：0.001 \n","Train Epoch: 6 [42000/60000 (70%)]\tLoss: 2.255915\t Lr：0.001 \n","Train Epoch: 6 [43000/60000 (72%)]\tLoss: 2.245954\t Lr：0.001 \n","Train Epoch: 6 [44000/60000 (73%)]\tLoss: 2.256773\t Lr：0.001 \n","Train Epoch: 6 [45000/60000 (75%)]\tLoss: 2.253188\t Lr：0.001 \n","Train Epoch: 6 [46000/60000 (77%)]\tLoss: 2.259111\t Lr：0.001 \n","Train Epoch: 6 [47000/60000 (78%)]\tLoss: 2.270773\t Lr：0.001 \n","Train Epoch: 6 [48000/60000 (80%)]\tLoss: 2.277042\t Lr：0.001 \n","Train Epoch: 6 [49000/60000 (82%)]\tLoss: 2.259281\t Lr：0.001 \n","Train Epoch: 6 [50000/60000 (83%)]\tLoss: 2.249499\t Lr：0.001 \n","Train Epoch: 6 [51000/60000 (85%)]\tLoss: 2.267444\t Lr：0.001 \n","Train Epoch: 6 [52000/60000 (87%)]\tLoss: 2.276277\t Lr：0.001 \n","Train Epoch: 6 [53000/60000 (88%)]\tLoss: 2.291383\t Lr：0.001 \n","Train Epoch: 6 [54000/60000 (90%)]\tLoss: 2.258903\t Lr：0.001 \n","Train Epoch: 6 [55000/60000 (92%)]\tLoss: 2.247672\t Lr：0.001 \n","Train Epoch: 6 [56000/60000 (93%)]\tLoss: 2.267740\t Lr：0.001 \n","Train Epoch: 6 [57000/60000 (95%)]\tLoss: 2.240022\t Lr：0.001 \n","Train Epoch: 6 [58000/60000 (97%)]\tLoss: 2.272380\t Lr：0.001 \n","Train Epoch: 6 [59000/60000 (98%)]\tLoss: 2.264927\t Lr：0.001 \n","\n","Test set: Average loss: 2.2445, Accuracy: 3458/10000 (35%)\n","\n","Train Epoch: 7 [0/60000 (0%)]\tLoss: 2.225902\t Lr：0.001 \n","Train Epoch: 7 [1000/60000 (2%)]\tLoss: 2.265305\t Lr：0.001 \n","Train Epoch: 7 [2000/60000 (3%)]\tLoss: 2.246572\t Lr：0.001 \n","Train Epoch: 7 [3000/60000 (5%)]\tLoss: 2.251104\t Lr：0.001 \n","Train Epoch: 7 [4000/60000 (7%)]\tLoss: 2.259277\t Lr：0.001 \n","Train Epoch: 7 [5000/60000 (8%)]\tLoss: 2.253418\t Lr：0.001 \n","Train Epoch: 7 [6000/60000 (10%)]\tLoss: 2.246488\t Lr：0.001 \n","Train Epoch: 7 [7000/60000 (12%)]\tLoss: 2.243593\t Lr：0.001 \n","Train Epoch: 7 [8000/60000 (13%)]\tLoss: 2.247506\t Lr：0.001 \n","Train Epoch: 7 [9000/60000 (15%)]\tLoss: 2.244555\t Lr：0.001 \n","Train Epoch: 7 [10000/60000 (17%)]\tLoss: 2.243409\t Lr：0.001 \n","Train Epoch: 7 [11000/60000 (18%)]\tLoss: 2.254184\t Lr：0.001 \n","Train Epoch: 7 [12000/60000 (20%)]\tLoss: 2.261760\t Lr：0.001 \n","Train Epoch: 7 [13000/60000 (22%)]\tLoss: 2.248235\t Lr：0.001 \n","Train Epoch: 7 [14000/60000 (23%)]\tLoss: 2.261839\t Lr：0.001 \n","Train Epoch: 7 [15000/60000 (25%)]\tLoss: 2.245732\t Lr：0.001 \n","Train Epoch: 7 [16000/60000 (27%)]\tLoss: 2.262617\t Lr：0.001 \n","Train Epoch: 7 [17000/60000 (28%)]\tLoss: 2.261879\t Lr：0.001 \n","Train Epoch: 7 [18000/60000 (30%)]\tLoss: 2.247231\t Lr：0.001 \n","Train Epoch: 7 [19000/60000 (32%)]\tLoss: 2.265377\t Lr：0.001 \n","Train Epoch: 7 [20000/60000 (33%)]\tLoss: 2.268001\t Lr：0.001 \n","Train Epoch: 7 [21000/60000 (35%)]\tLoss: 2.243146\t Lr：0.001 \n","Train Epoch: 7 [22000/60000 (37%)]\tLoss: 2.245120\t Lr：0.001 \n","Train Epoch: 7 [23000/60000 (38%)]\tLoss: 2.236750\t Lr：0.001 \n","Train Epoch: 7 [24000/60000 (40%)]\tLoss: 2.246266\t Lr：0.001 \n","Train Epoch: 7 [25000/60000 (42%)]\tLoss: 2.250260\t Lr：0.001 \n","Train Epoch: 7 [26000/60000 (43%)]\tLoss: 2.258081\t Lr：0.001 \n","Train Epoch: 7 [27000/60000 (45%)]\tLoss: 2.270096\t Lr：0.001 \n","Train Epoch: 7 [28000/60000 (47%)]\tLoss: 2.256137\t Lr：0.001 \n","Train Epoch: 7 [29000/60000 (48%)]\tLoss: 2.244232\t Lr：0.001 \n","Train Epoch: 7 [30000/60000 (50%)]\tLoss: 2.213641\t Lr：0.001 \n","Train Epoch: 7 [31000/60000 (52%)]\tLoss: 2.224017\t Lr：0.001 \n","Train Epoch: 7 [32000/60000 (53%)]\tLoss: 2.249719\t Lr：0.001 \n","Train Epoch: 7 [33000/60000 (55%)]\tLoss: 2.247505\t Lr：0.001 \n","Train Epoch: 7 [34000/60000 (57%)]\tLoss: 2.241664\t Lr：0.001 \n","Train Epoch: 7 [35000/60000 (58%)]\tLoss: 2.235858\t Lr：0.001 \n","Train Epoch: 7 [36000/60000 (60%)]\tLoss: 2.222317\t Lr：0.001 \n","Train Epoch: 7 [37000/60000 (62%)]\tLoss: 2.249623\t Lr：0.001 \n","Train Epoch: 7 [38000/60000 (63%)]\tLoss: 2.259396\t Lr：0.001 \n","Train Epoch: 7 [39000/60000 (65%)]\tLoss: 2.233814\t Lr：0.001 \n","Train Epoch: 7 [40000/60000 (67%)]\tLoss: 2.246323\t Lr：0.001 \n","Train Epoch: 7 [41000/60000 (68%)]\tLoss: 2.234586\t Lr：0.001 \n","Train Epoch: 7 [42000/60000 (70%)]\tLoss: 2.242071\t Lr：0.001 \n","Train Epoch: 7 [43000/60000 (72%)]\tLoss: 2.218054\t Lr：0.001 \n","Train Epoch: 7 [44000/60000 (73%)]\tLoss: 2.250952\t Lr：0.001 \n","Train Epoch: 7 [45000/60000 (75%)]\tLoss: 2.234252\t Lr：0.001 \n","Train Epoch: 7 [46000/60000 (77%)]\tLoss: 2.224734\t Lr：0.001 \n","Train Epoch: 7 [47000/60000 (78%)]\tLoss: 2.219818\t Lr：0.001 \n","Train Epoch: 7 [48000/60000 (80%)]\tLoss: 2.260459\t Lr：0.001 \n","Train Epoch: 7 [49000/60000 (82%)]\tLoss: 2.235346\t Lr：0.001 \n","Train Epoch: 7 [50000/60000 (83%)]\tLoss: 2.262044\t Lr：0.001 \n","Train Epoch: 7 [51000/60000 (85%)]\tLoss: 2.213567\t Lr：0.001 \n","Train Epoch: 7 [52000/60000 (87%)]\tLoss: 2.235072\t Lr：0.001 \n","Train Epoch: 7 [53000/60000 (88%)]\tLoss: 2.246601\t Lr：0.001 \n","Train Epoch: 7 [54000/60000 (90%)]\tLoss: 2.245908\t Lr：0.001 \n","Train Epoch: 7 [55000/60000 (92%)]\tLoss: 2.228328\t Lr：0.001 \n","Train Epoch: 7 [56000/60000 (93%)]\tLoss: 2.220126\t Lr：0.001 \n","Train Epoch: 7 [57000/60000 (95%)]\tLoss: 2.232455\t Lr：0.001 \n","Train Epoch: 7 [58000/60000 (97%)]\tLoss: 2.240279\t Lr：0.001 \n","Train Epoch: 7 [59000/60000 (98%)]\tLoss: 2.241677\t Lr：0.001 \n","\n","Test set: Average loss: 2.2065, Accuracy: 4929/10000 (49%)\n","\n","Train Epoch: 8 [0/60000 (0%)]\tLoss: 2.250180\t Lr：0.001 \n","Train Epoch: 8 [1000/60000 (2%)]\tLoss: 2.213741\t Lr：0.001 \n","Train Epoch: 8 [2000/60000 (3%)]\tLoss: 2.242450\t Lr：0.001 \n","Train Epoch: 8 [3000/60000 (5%)]\tLoss: 2.226516\t Lr：0.001 \n","Train Epoch: 8 [4000/60000 (7%)]\tLoss: 2.195970\t Lr：0.001 \n","Train Epoch: 8 [5000/60000 (8%)]\tLoss: 2.208241\t Lr：0.001 \n","Train Epoch: 8 [6000/60000 (10%)]\tLoss: 2.214213\t Lr：0.001 \n","Train Epoch: 8 [7000/60000 (12%)]\tLoss: 2.217633\t Lr：0.001 \n","Train Epoch: 8 [8000/60000 (13%)]\tLoss: 2.217674\t Lr：0.001 \n","Train Epoch: 8 [9000/60000 (15%)]\tLoss: 2.213849\t Lr：0.001 \n","Train Epoch: 8 [10000/60000 (17%)]\tLoss: 2.212161\t Lr：0.001 \n","Train Epoch: 8 [11000/60000 (18%)]\tLoss: 2.202560\t Lr：0.001 \n","Train Epoch: 8 [12000/60000 (20%)]\tLoss: 2.198194\t Lr：0.001 \n","Train Epoch: 8 [13000/60000 (22%)]\tLoss: 2.239748\t Lr：0.001 \n","Train Epoch: 8 [14000/60000 (23%)]\tLoss: 2.192470\t Lr：0.001 \n","Train Epoch: 8 [15000/60000 (25%)]\tLoss: 2.245701\t Lr：0.001 \n","Train Epoch: 8 [16000/60000 (27%)]\tLoss: 2.209206\t Lr：0.001 \n","Train Epoch: 8 [17000/60000 (28%)]\tLoss: 2.227494\t Lr：0.001 \n","Train Epoch: 8 [18000/60000 (30%)]\tLoss: 2.210812\t Lr：0.001 \n","Train Epoch: 8 [19000/60000 (32%)]\tLoss: 2.202879\t Lr：0.001 \n","Train Epoch: 8 [20000/60000 (33%)]\tLoss: 2.201873\t Lr：0.001 \n","Train Epoch: 8 [21000/60000 (35%)]\tLoss: 2.224835\t Lr：0.001 \n","Train Epoch: 8 [22000/60000 (37%)]\tLoss: 2.188252\t Lr：0.001 \n","Train Epoch: 8 [23000/60000 (38%)]\tLoss: 2.240349\t Lr：0.001 \n","Train Epoch: 8 [24000/60000 (40%)]\tLoss: 2.178228\t Lr：0.001 \n","Train Epoch: 8 [25000/60000 (42%)]\tLoss: 2.205637\t Lr：0.001 \n","Train Epoch: 8 [26000/60000 (43%)]\tLoss: 2.229721\t Lr：0.001 \n","Train Epoch: 8 [27000/60000 (45%)]\tLoss: 2.228454\t Lr：0.001 \n","Train Epoch: 8 [28000/60000 (47%)]\tLoss: 2.220110\t Lr：0.001 \n","Train Epoch: 8 [29000/60000 (48%)]\tLoss: 2.179703\t Lr：0.001 \n","Train Epoch: 8 [30000/60000 (50%)]\tLoss: 2.222900\t Lr：0.001 \n","Train Epoch: 8 [31000/60000 (52%)]\tLoss: 2.211391\t Lr：0.001 \n","Train Epoch: 8 [32000/60000 (53%)]\tLoss: 2.195977\t Lr：0.001 \n","Train Epoch: 8 [33000/60000 (55%)]\tLoss: 2.158314\t Lr：0.001 \n","Train Epoch: 8 [34000/60000 (57%)]\tLoss: 2.218775\t Lr：0.001 \n","Train Epoch: 8 [35000/60000 (58%)]\tLoss: 2.218389\t Lr：0.001 \n","Train Epoch: 8 [36000/60000 (60%)]\tLoss: 2.199483\t Lr：0.001 \n","Train Epoch: 8 [37000/60000 (62%)]\tLoss: 2.214538\t Lr：0.001 \n","Train Epoch: 8 [38000/60000 (63%)]\tLoss: 2.232703\t Lr：0.001 \n","Train Epoch: 8 [39000/60000 (65%)]\tLoss: 2.213194\t Lr：0.001 \n","Train Epoch: 8 [40000/60000 (67%)]\tLoss: 2.163120\t Lr：0.001 \n","Train Epoch: 8 [41000/60000 (68%)]\tLoss: 2.174626\t Lr：0.001 \n","Train Epoch: 8 [42000/60000 (70%)]\tLoss: 2.182444\t Lr：0.001 \n","Train Epoch: 8 [43000/60000 (72%)]\tLoss: 2.196625\t Lr：0.001 \n","Train Epoch: 8 [44000/60000 (73%)]\tLoss: 2.187889\t Lr：0.001 \n","Train Epoch: 8 [45000/60000 (75%)]\tLoss: 2.200642\t Lr：0.001 \n","Train Epoch: 8 [46000/60000 (77%)]\tLoss: 2.223888\t Lr：0.001 \n","Train Epoch: 8 [47000/60000 (78%)]\tLoss: 2.187394\t Lr：0.001 \n","Train Epoch: 8 [48000/60000 (80%)]\tLoss: 2.209958\t Lr：0.001 \n","Train Epoch: 8 [49000/60000 (82%)]\tLoss: 2.185599\t Lr：0.001 \n","Train Epoch: 8 [50000/60000 (83%)]\tLoss: 2.188251\t Lr：0.001 \n","Train Epoch: 8 [51000/60000 (85%)]\tLoss: 2.183878\t Lr：0.001 \n","Train Epoch: 8 [52000/60000 (87%)]\tLoss: 2.170655\t Lr：0.001 \n","Train Epoch: 8 [53000/60000 (88%)]\tLoss: 2.167315\t Lr：0.001 \n","Train Epoch: 8 [54000/60000 (90%)]\tLoss: 2.152619\t Lr：0.001 \n","Train Epoch: 8 [55000/60000 (92%)]\tLoss: 2.186274\t Lr：0.001 \n","Train Epoch: 8 [56000/60000 (93%)]\tLoss: 2.140950\t Lr：0.001 \n","Train Epoch: 8 [57000/60000 (95%)]\tLoss: 2.206667\t Lr：0.001 \n","Train Epoch: 8 [58000/60000 (97%)]\tLoss: 2.160273\t Lr：0.001 \n","Train Epoch: 8 [59000/60000 (98%)]\tLoss: 2.186456\t Lr：0.001 \n","\n","Test set: Average loss: 2.1340, Accuracy: 5675/10000 (57%)\n","\n","Train Epoch: 9 [0/60000 (0%)]\tLoss: 2.183539\t Lr：0.001 \n"],"name":"stdout"},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-14-8b8d33bf0b1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     26\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_list\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loss_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcorrect_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-14-8b8d33bf0b1c>\u001b[0m in \u001b[0;36mmain\u001b[0;34m(optimizer, model)\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mref_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m         \u001b[0mtrain_loss_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mref_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m         \u001b[0mref_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mref_correct\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdevice\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m<ipython-input-3-0c4dd3557b12>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, model, device, train_loader, optimizer, epoch, use_scheduler)\u001b[0m\n\u001b[1;32m     42\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0muse_scheduler\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscheduler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m         \u001b[0;32mfor\u001b[0m \u001b[0mbatch_idx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m             \u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#勾配クリア\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_workers\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# same-process loading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m             \u001b[0mindices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample_iter\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcollate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mindices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m                 \u001b[0mbatch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpin_memory_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, pic)\u001b[0m\n\u001b[1;32m     74\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mConverted\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m         \"\"\"\n\u001b[0;32m---> 76\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_tensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mto_tensor\u001b[0;34m(pic)\u001b[0m\n\u001b[1;32m     76\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m         \u001b[0mnchannel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m     \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnchannel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m     \u001b[0;31m# put it from HWC to CHW format\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;31m# yikes, this transpose takes 80% of the loading time/CPU\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}]},{"metadata":{"id":"hgY8lpw2n_Uk","colab_type":"code","colab":{}},"cell_type":"code","source":[""],"execution_count":0,"outputs":[]}]}